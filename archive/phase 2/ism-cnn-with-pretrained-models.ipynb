{"cells":[{"cell_type":"code","execution_count":99,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-01-18T05:30:27.775911Z","iopub.status.busy":"2023-01-18T05:30:27.775182Z","iopub.status.idle":"2023-01-18T05:30:27.804820Z","shell.execute_reply":"2023-01-18T05:30:27.804008Z","shell.execute_reply.started":"2023-01-18T05:30:27.775791Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","#import numpy as np # linear algebra\n","#import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","#import os\n","#for dirname, _, filenames in os.walk('/kaggle/input'):\n","#    for filename in filenames:\n","#        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","metadata":{},"source":["# Warnings"]},{"cell_type":"code","execution_count":100,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:27.892232Z","iopub.status.busy":"2023-01-18T05:30:27.891550Z","iopub.status.idle":"2023-01-18T05:30:27.900561Z","shell.execute_reply":"2023-01-18T05:30:27.899544Z","shell.execute_reply.started":"2023-01-18T05:30:27.892200Z"},"trusted":true},"outputs":[],"source":["# To ignore warinings\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{},"source":["# Import Necessary Libraries"]},{"cell_type":"code","execution_count":101,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:27.989720Z","iopub.status.busy":"2023-01-18T05:30:27.988889Z","iopub.status.idle":"2023-01-18T05:30:31.146649Z","shell.execute_reply":"2023-01-18T05:30:31.145568Z","shell.execute_reply.started":"2023-01-18T05:30:27.989686Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import cv2 \n","import os\n","\n","from collections import OrderedDict\n","\n","import torch\n","from torch import nn\n","from torchvision.io import read_image,ImageReadMode\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, models\n","\n","from sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit, StratifiedKFold, KFold\n","from sklearn import preprocessing\n","from sklearn.metrics import matthews_corrcoef, accuracy_score, balanced_accuracy_score, precision_score, f1_score, recall_score\n","\n","from matplotlib import pyplot as plt\n","\n","from sklearn.metrics import ConfusionMatrixDisplay\n","import matplotlib.image as mpimg\n","\n","import json\n","\n","import pandas as pd\n","\n","import time\n","import datetime\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Data Preprocessing"]},{"cell_type":"markdown","metadata":{},"source":["* load training labels.txt file and split the data set into train and valid dataset using simple Stratified or kfold stratified.\n","* Setup Dataset and Dataloader in Pytorch for loading training, validation, test and noisy dataset and for applying transformations and data augmentation\n","\n","Transformations (examples):\n","   * Normalize\n","   * resize\n","   * zero centering (AlexNet, VGG etc) during training and depedning the method, save values calculated during training and use them to apply on validation, testing and training\n","   * denoise the image (if required...how to know that?)\n","       \n","Data Augmentation (examples):\n","   * cropping, scaling images, similar to ResNet,or maybe something else etc (which ones to use will depend upon type of images we expect in the real world...or in our project in noisy test/test dataset)\n","   * If we know somehow what kind of noise will be there in noisy_test/test dataset (e.g. gaussion noise, salt and pepper, etc) then we can add something similar to training dataset to create more images and use them to train the model as well\n","\n","Make use of GPU\n","\n","Save all the hyperparameters somewhere. "]},{"cell_type":"markdown","metadata":{},"source":["## Split the data"]},{"cell_type":"code","execution_count":102,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.149746Z","iopub.status.busy":"2023-01-18T05:30:31.149146Z","iopub.status.idle":"2023-01-18T05:30:31.164386Z","shell.execute_reply":"2023-01-18T05:30:31.162975Z","shell.execute_reply.started":"2023-01-18T05:30:31.149704Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def split_data(y, split_type, test_size=0.3, n_folds=5):\n","    \"\"\"\n","    Split the datasets\n","\n","    Parameters:\n","        y: array of labels and image ids with shape (num_images, 2)\n","        split_type: type of splitting (\"simple\", \"simpleStratified\", \"kfold\", \"kfoldStratified\")\n","        test_size: fraction of data for testing (default=0.3)\n","        n_folds: number of folds (default=5)\n","    Returns:\n","        train_labels: numpy array of shape(folds, num_images, 2)\n","        valid_labels: numpy array of shape(folds, num_images, 2)\n","    \n","    \"\"\"\n","\n","    if split_type==\"simple\":\n","        \n","        print(f\"Splitting data using Simple {100-test_size*100}-{test_size*100} ratio\")\n","       \n","        # shuffle data before split\n","        ss = ShuffleSplit(n_splits=1, test_size=test_size)\n","\n","        for train_index, valid_index in ss.split(X=np.ones(len(y))):\n","            y_train = np.expand_dims(y[train_index], axis=0)\n","            y_valid = np.expand_dims(y[valid_index], axis=0)\n","    \n","    elif split_type==\"simpleStratified\":\n","        \n","        \n","        print(f\"Splitting data using Simple Stratified with {100-test_size*100}-{test_size*100} ratio\")\n","\n","        # shuffle data before split\n","        ss = StratifiedShuffleSplit(n_splits=1, test_size=test_size)\n","\n","        for train_index, valid_index in ss.split(X=np.ones(len(y)), y=y[:,1]):\n","            y_train = np.expand_dims(y[train_index], axis=0)\n","            y_valid = np.expand_dims(y[valid_index], axis=0)\n","\n","    \n","    elif split_type==\"kfold\":\n","        \n","        print(f\"Splitting data using kfolds with number of folds: {n_folds}\")\n","\n","        # shuffle data before creating folds\n","        kf = KFold(n_splits=n_folds, shuffle=True)\n","        \n","        y_train = []\n","        y_valid = []\n","        for train_index, valid_index in kf.split(X=np.ones(len(y))):\n","            y_train.append(y[train_index])\n","            y_valid.append(y[valid_index])\n","        \n","        y_train = np.array(y_train)\n","        y_valid = np.array(y_valid)\n","\n","    \n","    elif split_type == \"kfoldStratified\":\n","        \n","        \n","        print(f\"Splitting data using kfold Stratfication with number of folds: {n_folds}\")\n","\n","        # shuffle data before creating folds\n","        skf = StratifiedKFold(n_splits=n_folds, shuffle=True)\n","        y_train = []\n","        y_valid = []\n","        \n","        for train_index, valid_index in skf.split(X=np.ones(len(y)), y=y[:,1]):\n","\n","            y_train.append(y[train_index])\n","            y_valid.append(y[valid_index])\n","\n","        y_train = np.array(y_train)\n","        y_valid = np.array(y_valid)\n","\n","    else:\n","        raise ValueError(f\"Unknown value encountered for the parameter 'split_type' during splitting of the data. received {split_type}\")\n","\n","    return y_train, y_valid"]},{"cell_type":"code","execution_count":103,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.166896Z","iopub.status.busy":"2023-01-18T05:30:31.166525Z","iopub.status.idle":"2023-01-18T05:30:31.177356Z","shell.execute_reply":"2023-01-18T05:30:31.176458Z","shell.execute_reply.started":"2023-01-18T05:30:31.166861Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["\n","#path_to_labels = \"/kaggle/input/ismdatasetforclassificationofdieases/code_testing/code_testing/train_binary.txt\"\n","#split_type = \"simple\"\n","#test_size= 0.2\n","#n_folds = 4\n","#img_ids_labels = np.loadtxt(path_to_labels, dtype=str, delimiter=\" \")\n","#y_train, y_valid = split_data(y=img_ids_labels, split_type=split_type, test_size=test_size, n_folds=n_folds)  \n","#print(\"train\")\n","#print(y_train.shape)\n","#print(\"valid\")\n","#print(y_valid.shape)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Encode labels"]},{"cell_type":"code","execution_count":104,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.180660Z","iopub.status.busy":"2023-01-18T05:30:31.180304Z","iopub.status.idle":"2023-01-18T05:30:31.187616Z","shell.execute_reply":"2023-01-18T05:30:31.186496Z","shell.execute_reply.started":"2023-01-18T05:30:31.180624Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def label_encoder(y, classes, to_numbers):\n","    \"\"\"\n","    encodes target labels with value between 0 and n_classes-1 and vice versa\n","\n","    Parameter:\n","        y: labels of numpy array (num_images,)\n","        classes: numpy array of names of classes\n","        to_numbers True/False: whether to transfer from string to numbers or vice versa\n","\n","    Returns:\n","        result: encoded labels of numpy array (num_images, 1)\n","                or labels of numpy array (num_images, 1)\n","    \"\"\"\n","\n","    le = preprocessing.LabelEncoder()\n","    \n","    if type(classes).__name__ == 'list':\n","        classes = np.array(classes)\n","    le.classes_ = classes\n","    \n","    if to_numbers:\n","        return le.transform(y.ravel())\n","    else:\n","        return le.inverse_transform(y.ravel().astype(int)) \n"]},{"cell_type":"code","execution_count":105,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.189951Z","iopub.status.busy":"2023-01-18T05:30:31.189449Z","iopub.status.idle":"2023-01-18T05:30:31.196450Z","shell.execute_reply":"2023-01-18T05:30:31.195482Z","shell.execute_reply.started":"2023-01-18T05:30:31.189914Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["#classes = np.array([\"Normal\", \"COVID\", \"pneumonia\", \"Lung_Opacity\"]) \n","#classes = np.array([\"NO_COVID\", \"COVID\"])\n","#for nf in range(n_folds):\n","#    y_train[nf, :, 1] = label_encoder(y=y_train[nf,:,1], classes=classes, to_numbers=True)\n","#    y_valid[nf, :, 1] = label_encoder(y=y_valid[nf,:,1], classes=classes, to_numbers=True)\n","\n","#print(\"y-train: \", y_train.shape)\n","#print(\"y-valid: \", y_valid.shape)"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset "]},{"cell_type":"code","execution_count":106,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.198445Z","iopub.status.busy":"2023-01-18T05:30:31.198086Z","iopub.status.idle":"2023-01-18T05:30:31.210778Z","shell.execute_reply":"2023-01-18T05:30:31.209890Z","shell.execute_reply.started":"2023-01-18T05:30:31.198397Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["class ImageDataset(Dataset):\n","    \"\"\"\n","    \n","    Parameters:\n","        labels: labels of shape(num_images, 2) or (num_images, 1)\n","        path_to_images: path to the folder containing the images\n","        color: whether to read image in \"gray\" or \"rgb\" (default=\"gray\")\n","        transform: transform to be applied on a sample\n","        \n","        When combined with dataloader, it returns:\n","            image: tensor of shape (batch, 1, height, width)\n","            label: tensor of shape (batch)\n","    \n","    \"\"\"\n","    def __init__(self, labels, path_to_images, color=\"gray\", transform=None):\n","        self.img_labels = labels\n","        self.img_dir = path_to_images\n","        self.transform = transform\n","        \n","        if color ==\"gray\":\n","            self.color = ImageReadMode.GRAY\n","        elif color ==\"rgb\":\n","            self.color = ImageReadMode.RGB\n","        else:\n","            raise ValueError(\"unknown color passed to dataset\")\n","        \n","\n","    def __len__(self):\n","        return self.img_labels.shape[0]\n","\n","    def __getitem__(self, index):\n","        img_path = os.path.join(self.img_dir, self.img_labels[index, 0])\n","        \n","        # read image \n","        image = read_image(img_path, self.color).float()\n","        \n","        if self.img_labels.shape[-1]==2:\n","            label = self.img_labels[index, 1].astype(int)\n","        else:\n","            label = -1.0 # just place holder for valid data\n","            \n","        if self.transform:\n","            image = self.transform(image)\n","            \n","        return image, label"]},{"cell_type":"code","execution_count":107,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.212516Z","iopub.status.busy":"2023-01-18T05:30:31.212110Z","iopub.status.idle":"2023-01-18T05:30:31.221223Z","shell.execute_reply":"2023-01-18T05:30:31.220303Z","shell.execute_reply.started":"2023-01-18T05:30:31.212481Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["#path = \"/kaggle/input/ismdatasetforclassificationofdieases/code_testing/code_testing/train/0a01d14b-2c8b-4155-ae95-095f625315bd.png\"\n","#image = read_image(path, ImageReadMode.GRAY)\n","#print(image.size())\n","#plt.imshow(image[0])"]},{"cell_type":"code","execution_count":108,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.223092Z","iopub.status.busy":"2023-01-18T05:30:31.222607Z","iopub.status.idle":"2023-01-18T05:30:31.234349Z","shell.execute_reply":"2023-01-18T05:30:31.233470Z","shell.execute_reply.started":"2023-01-18T05:30:31.223056Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["#image2 = read_image(path, ImageReadMode.RGB)\n","#print(image2.size())\n","#plt.imshow(image2.permute(1,2,0))"]},{"cell_type":"code","execution_count":109,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.236669Z","iopub.status.busy":"2023-01-18T05:30:31.235737Z","iopub.status.idle":"2023-01-18T05:30:31.245020Z","shell.execute_reply":"2023-01-18T05:30:31.243967Z","shell.execute_reply.started":"2023-01-18T05:30:31.236635Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["#import PIL as p\n","#img2 = p.Image.open(path).convert('RGB')\n","#print(img2.size)\n","#plt.imshow(img2)"]},{"cell_type":"code","execution_count":110,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.249555Z","iopub.status.busy":"2023-01-18T05:30:31.249303Z","iopub.status.idle":"2023-01-18T05:30:31.255093Z","shell.execute_reply":"2023-01-18T05:30:31.254050Z","shell.execute_reply.started":"2023-01-18T05:30:31.249532Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["#trans = transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x)\n","#trans_image = trans(image)\n","#print(trans_image.size())\n","#plt.imshow(trans_image.permute(1,2,0))"]},{"cell_type":"code","execution_count":111,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.257862Z","iopub.status.busy":"2023-01-18T05:30:31.257139Z","iopub.status.idle":"2023-01-18T05:30:31.265169Z","shell.execute_reply":"2023-01-18T05:30:31.264289Z","shell.execute_reply.started":"2023-01-18T05:30:31.257806Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["#np.sum(trans_image.numpy() == image2.numpy())"]},{"cell_type":"markdown","metadata":{},"source":["data preprocessing"]},{"cell_type":"code","execution_count":112,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.268879Z","iopub.status.busy":"2023-01-18T05:30:31.268597Z","iopub.status.idle":"2023-01-18T05:30:31.285192Z","shell.execute_reply":"2023-01-18T05:30:31.284301Z","shell.execute_reply.started":"2023-01-18T05:30:31.268822Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def data_preprocessing(transformations):\n","    \"\"\"\n","    Returns compose of transforms\n","    \n","    Parameters:\n","        transformations: Ordered dict with structure\n","        transformations[\"transform_1\"] = {}\n","        transformations[\"transform_1\"][\"name\"] = value\n","        transformations[\"transform_1\"][\"parameter_1\"] = value\n","        transformations[\"transform_1\"][\"parameter_2\"] = value\n","        \n","        transformations[\"transform_2\"] = {}\n","        transformations[\"transform_2\"][\"name\"] = value\n","        transformations[\"transform_2\"][\"parameter_1\"] = value\n","        transformations[\"transform_2\"][\"parameter_2\"] = value\n","        \n","    Currently supports transformation: \n","        \"normalize\": \n","                    keys: mean (default=0) \n","                           std (default=255), \n","        \"resize\": \n","                    keys: output_shape, \n","        \n","        \"random_resized_crop\"\n","                    keys: output_shape\n","        \n","        \"random_horizontal_flip\"\n","                    keys: p = 0.5\n","        \n","        \"random_vectical_flip\"\n","                    keys: p = 0.5           \n","        \n","        \"center_crop\"\n","                    keys: output_shape\n","                    \n","        \"random_rotation\"\n","                    keys: degrees (defines the range to select from (-deg, +deg)) # in degrees\n","                        expand(default=False, whether to increase the size of image so that it can fit whole roatated image)\n","                        \n","    \n","    # train transform will be different from valid transform (CHECK)\n","    \"\"\"\n","    \n","    preprocess = []\n","    \n","    for tran in transformations.keys():\n","        tran_name = transformations[tran][\"name\"]\n","        \n","        # normalization\n","        if tran_name == \"normalize\":\n","            norm = transformations[tran].keys()\n","            if \"mean\" in norm:\n","                m = transformations[tran][\"mean\"]\n","                if type(m) == list:\n","                    mean = m\n","                else:\n","                    mean = [m]\n","            else:\n","                mean = [0]\n","            \n","            if \"std\" in norm:\n","                s =transformations[tran][\"std\"]\n","                if type(s) == list:\n","                    std = s\n","                else:\n","                    std = [s]\n","            else:\n","                std = [255]\n","                \n","            preprocess.append(transforms.Normalize(mean=mean,\n","                             std=std))\n","        # resize\n","        elif tran_name == \"resize\":\n","            resize = transformations[tran].keys()\n","            \n","            if \"output_shape\" in resize:\n","                output_shape = transformations[tran][\"output_shape\"]\n","            else:\n","                raise ValueError(\"Output shape must be provided while resizing\")\n","            \n","            preprocess.append(transforms.Resize(output_shape))\n","            \n","        # random resize crop\n","        elif tran_name == \"random_resized_crop\":\n","            rrc = transformations[tran].keys()\n","            \n","            if \"output_shape\" in rrc:\n","                output_shape = transformations[tran][\"output_shape\"]\n","            else:\n","                raise ValueError(\"Output shape must be provided while random_resized_crop\") \n","            \n","            preprocess.append(transforms.RandomResizedCrop(output_shape))\n","        \n","        # random horizontal flip\n","        elif tran_name == \"random_horizontal_flip\":\n","            rhf = transformations[tran].keys()\n","            \n","            if \"p\" in rhf:\n","                p = transformations[tran][\"p\"]\n","            else:\n","                p = 0.5\n","            \n","            preprocess.append(transforms.RandomHorizontalFlip(p))\n","         \n","        # random vertical flip\n","        elif tran_name == \"random_vertical_flip\":\n","            rvf = transformations[tran].keys()\n","            \n","            if \"p\" in rvf:\n","                p = transformations[tran][\"p\"]\n","            else:\n","                p = 0.5\n","            \n","            preprocess.append(transforms.RandomVerticalFlip(p))\n","            \n","        # center crop\n","        elif tran_name == \"center_crop\":\n","            cc = transformations[tran].keys()\n","            \n","            if \"output_shape\" in cc:\n","                output_shape = transformations[tran][\"output_shape\"]\n","            else:\n","                raise ValueError(\"Output shape must be provided while center_crop\") \n","            \n","            preprocess.append(transforms.CenterCrop(output_shape))\n","        \n","        # random_rotation\n","        elif tran_name == \"random_rotation\":\n","            rr = transformations[tran].keys()\n","            \n","            if \"degrees\" in rr:\n","                degrees = transformations[tran][\"degrees\"]\n","            else:\n","                raise ValueError(\"degrees must be provided while random rotation\")\n","            \n","            if \"expand\" in rr:\n","                expand = transformations[tran][\"expand\"]\n","            else:\n","                expand = False\n","            \n","            preprocess.append(transforms.RandomRotation(degrees=degrees, expand=expand))\n","        \n","        else:\n","            raise ValueError(\"Unknown transformation passed\")\n","    \n","    return transforms.Compose(preprocess)"]},{"cell_type":"code","execution_count":113,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.287460Z","iopub.status.busy":"2023-01-18T05:30:31.286494Z","iopub.status.idle":"2023-01-18T05:30:31.299703Z","shell.execute_reply":"2023-01-18T05:30:31.298718Z","shell.execute_reply.started":"2023-01-18T05:30:31.287426Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["\n","#path_to_images = \"/kaggle/input/ismdatasetforclassificationofdieases/code_testing/code_testing/train\"\n","#batch_size = 2\n","\n","#data_transform = OrderedDict()\n","#data_transform[\"t1\"] = {}\n","#data_transform[\"t1\"][\"name\"] = \"normalize\"\n","#data_transform[\"t1\"][\"mean\"] = 0\n","#data_transform[\"t1\"][\"std\"] = 255\n","#data_transform[\"t2\"] = {}\n","#data_transform[\"t2\"][\"name\"] = \"resize\"\n","#data_transform[\"t2\"][\"output_shape\"] = (5,5)\n","\n","\n","#data_transforms = data_preprocessing(data_transform)\n","\n","#data_transforms = transforms.Compose([\n","#        transforms.RandomSizedCrop(224),\n","#        transforms.RandomHorizontalFlip(),\n","#        transforms.ToTensor(),\n","#        transforms.Normalize(mean=[0],\n","#                             std=[255]),\n","#        transforms.Resize((250,250))\n","#    ])\n","\n","\n","\n","#for fold in range(n_folds):\n","\n","    #train_data = ImageDataset(y_train[fold], path_to_images, transform=data_transforms)\n","    #train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","#    print(len(train_dataloader))\n"," #   for x, y in train_dataloader:\"\"\"\n","  #  #print(f\"Fold No: {fold} Train: x shape: {x.shape} y_shape: {y.shape}\")\n","   # \"\"\"\n","   # valid_data = ImageDataset(y_valid[fold], path_to_images, transform=data_transforms)\n","   # valid_dataloader = DataLoader(valid_data, batch_size=batch_size, shuffle=False)\n","  #  print(len(valid_dataloader))\n","  #  for x, y in valid_dataloader:\"\"\"\n","#print(f\"Fold No: {fold} Valid: x shape: {x.shape} y_shape: {y.shape}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Setup Model"]},{"cell_type":"markdown","metadata":{},"source":["Model setup could be done using the following ways:\n","   1. Model with user defined layers.\n","   2. Pretrained Model\n","   3. Ensemble of models\n","\n","Layers could be:\n","   * Convolution\n","   * Activation (Linear, ReLU, LReLU, PReLU, tanh, etc)\n","   * Pooling (Max, Average, Global)\n","   * Fully Connected Layer\n","   * Dropout\n","   * Batch Normalization\n","and many others\n","\n","These layers could be added to the any model. Which layers to be used can be find by hit and trial method or from literature\n","\n","Pretrained Model could be:\n","   * VGG\n","   * ResNet\n","   * GoogleLeNet\n","and many others.\n","\n","Which pretrained model to use can be find by literature and how many layers to be trained can be determined by hit and trial or from literature.\n","\n","We can always combine pretrained model with own custom model.\n","\n","Make use of GPU\n","\n","Save the model, and all the hyperparamters and all the info that may be learned during training and may be required in testing phase"]},{"cell_type":"code","execution_count":114,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.301792Z","iopub.status.busy":"2023-01-18T05:30:31.301258Z","iopub.status.idle":"2023-01-18T05:30:31.338224Z","shell.execute_reply":"2023-01-18T05:30:31.337380Z","shell.execute_reply.started":"2023-01-18T05:30:31.301761Z"},"trusted":true},"outputs":[],"source":["class Model(nn.Module):\n","    \"\"\"\n","    Sets up the model\n","    layers: OrderedDict with format:\n","        layers[\"layer_1\"] = {}\n","        layers[\"layer_1\"][\"name\"] = value\n","        layers[\"layer_1\"][\"parameter1\"] = value\n","        layers[\"layer_1\"][\"parameter2\"] = value\n","        \n","        layers[\"layer_2\"] = {}\n","        layers[\"layer_2\"][\"name\"] = value\n","        layers[\"layer_2\"][\"parameter1\"] = value\n","        layers[\"layer_2\"][\"parameter2\"] = value\n","        \n","    currently supports layer names. \"linear\", \"flatten\", \"relu\", \"lrelu\"\n","    \n","    output layer must be added as well\n","    \"\"\"\n","    def __init__(self, input_shape, layers, use_pretrained_model):\n","        super(Model, self).__init__()\n","        self.input_shape = input_shape\n","        self.layers = layers\n","        if use_pretrained_model:\n","            self.seq_model = self.get_pretrained_model()\n","        else:\n","            self.seq_model = nn.Sequential(self.get_model())\n","    \n","    def forward(self, x):\n","        return self.seq_model(x)\n","    \n","    def get_model(self):\n","        \n","        arch = []\n","        self.layer = \"Init\"\n","        self.pre_layer_output = self.input_shape  \n","        \n","        for layer in self.layers.keys():\n","            self.layer = layer\n","            layer_name = self.layers[layer][\"name\"] \n","            \n","            # Linear Layer \n","            if layer_name == \"linear\":\n","                arch.append(self.add_linear(self.layers[layer]))\n","                \n","            # ReLU\n","            elif layer_name == \"relu\":\n","                arch.append(self.add_relu(self.layers[layer]))\n","                \n","            # LeakyReLU\n","            elif layer_name == \"lrelu\":\n","                arch.append(self.add_lrelu(self.layers[layer]))\n","            \n","            # flatten\n","            elif layer_name == \"flatten\":\n","                arch.append(self.add_flatten(self.layers[layer]))\n","                \n","            # conv\n","            elif layer_name == \"conv\":\n","                arch.append(self.add_conv(self.layers[layer]))\n","            \n","            # max pooling\n","            elif layer_name == \"max_pool\":\n","                arch.append(self.add_max_pool(self.layers[layer]))\n","            \n","            else:\n","                raise ValueError(\"Unknown layer encountered\")\n","\n","        return OrderedDict(arch)\n","    \n","    def get_pretrained_model(self):\n","        \n","        if len(self.layers.keys()) != 1:\n","            raise ValueError(\"Currently only single pretrained model is supported\")\n","        \n","        for layer in self.layers.keys():\n","            self.layer = layer\n","            layer_name = self.layers[layer][\"name\"] \n","\n","            # mobile_net \n","            if layer_name == \"mobile_net\":\n","                return self.add_mobile_net(self.layers[layer])\n","\n","            # resnet18\n","            elif layer_name == \"resnet\":\n","                return self.add_resnet(self.layers[layer])\n","            \n","            # resnet50\n","            elif layer_name == \"resnet50\":\n","                return self.add_resnet50(self.layers[layer])\n","\n","            else:\n","                raise ValueError(\"Unknown pretrained model name encountered\")\n","    \n","    def set_parameter_requires_grad(self, features, flag):\n","        \"\"\"\n","        Sets requires grad of layers to True/False\n","        \"\"\"\n","        for param in features.parameters():\n","            param.requires_grad = flag\n","\n","    def add_linear(self, parameters):\n","        \"\"\"\n","        Adds Linear Layer\n","        \n","        parameters: dict containing keys:\n","            neurons: number of neurons in layer\n","            bias: whether to add bias term (default = True)\n","        \"\"\"\n","        param = parameters.keys()\n","        \n","        if \"neurons\" in param:\n","            neurons = parameters[\"neurons\"]\n","        else:\n","            raise ValueError (\"Number of neurons must be provided\")\n","            \n","        if \"bias\" in param:\n","            bias = parameters[\"bias\"]\n","        else:\n","            bias = True\n","            \n","        linear = (f'{self.layer}', nn.Linear(\n","                                    in_features = self.pre_layer_output, \n","                                    out_features= neurons, \n","                                    bias = bias))\n","        self.pre_layer_output = neurons\n","        \n","        return linear\n","    \n","    def add_relu(self, parameters):\n","        \"\"\"\n","        Adds ReLU layer\n","        \"\"\"\n","        return (f'{self.layer}', nn.ReLU())\n","    \n","    def add_lrelu(self, parameters):\n","        \"\"\"\n","        Adds ReLU layer\n","        Parameters:\n","            alpha: negative slope (default = 1e-2)\n","        \"\"\"\n","        if \"alpha\" in parameters.keys():\n","            alpha = parameters[\"alpha\"]\n","        else:\n","            alpha = 1e-2\n","        return (f'{self.layer}', nn.LeakyReLU(alpha))\n","    \n","    def add_flatten(self, parameters):\n","        \"\"\"\n","        Adds Flatten layer\n","        \"\"\"\n","        flatten_layer = (f'{self.layer}', nn.Flatten())\n","        in_shape = self.pre_layer_output\n","        \n","        # calculate output shape\n","        out_shape = 1\n","        for d in range(len(in_shape)):\n","            out_shape *= in_shape[d]\n","        \n","        self.pre_layer_output = out_shape\n","        \n","        return flatten_layer\n","    \n","    def add_conv(self, parameters):\n","        \"\"\"\n","        Adds Conv layer\n","        \n","        Parameters:\n","            number_of_kernels: int\n","            kernel_size: int or tuple of ints\n","            stride: int or tuple of ints (default=1)\n","            padding: 'valid' or 'same' or tuple of ints (default='valid')\n","            bias: default=True\n","            \n","        \"\"\"\n","        param_keys = parameters.keys()\n","        \n","        if \"number_of_kernels\" in param_keys:\n","            out_channels = parameters[\"number_of_kernels\"]\n","        else:\n","            raise ValueError(\"When adding Convulational Layer, kernel_size must be provided\")\n","        \n","        \n","        if \"kernel_size\" in param_keys:\n","            kernel_size = parameters[\"kernel_size\"]\n","        else:\n","            raise ValueError(\"When adding Convulational Layer, kernel_size must be provided\")\n","        \n","        if \"stride\" in param_keys:\n","            stride = parameters[\"stride\"]\n","        else:\n","            stride = 1\n","            \n","        if \"padding\" in param_keys:\n","            padding = parameters[\"padding\"]\n","        else:\n","            padding = 'valid'\n","        \n","        if \"bias\" in param_keys:\n","            bias = parameters[\"bias\"]\n","        else:\n","            bias = True\n","        \n","        if len(self.pre_layer_output) == 2:\n","            in_channels = 1\n","            in_height, in_width =  self.pre_layer_output\n","        else:\n","            in_channels, in_height, in_width =  self.pre_layer_output\n","        \n","        if padding == 'valid':\n","            pad = 0\n","        \n","        if padding == 'same':\n","            pad =  ( (in_height - 1)* stride + kernel_size - in_height)/2\n","       \n","        \n","        out_height = (in_height + 2*pad - kernel_size)/ stride + 1\n","        out_width = (in_width + 2*pad - kernel_size)/ stride + 1\n","        \n","        if out_width %1 !=0 or out_height %1 !=0 :\n","            raise ValueError(f\"Combination of pad, stride and kernel size lead to decimal number in conv layer. {out_height}\")\n","\n","        self.pre_layer_output = (out_channels, int(out_height), int(out_width))\n","        \n","        conv_layer = (f'{self.layer}', nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias))\n","        \n","        return conv_layer\n","    \n","    def add_max_pool(self, parameters):\n","        \"\"\"\n","        Adds Max pooling layer\n","        \n","        Parameters:\n","            kernel_size: int or tuple of ints\n","            stride: int or tuple of ints (default=2)\n","            padding:int or tuple of ints (default=0)\n","        \n","        \"\"\"\n","        \n","        param_keys = parameters.keys()\n","        \n","        if \"kernel_size\" in param_keys:\n","            kernel_size = parameters[\"kernel_size\"]\n","        else:\n","            raise ValueError(\"When adding Convulational Layer, kernel_size must be provided\")\n","        \n","        if \"stride\" in param_keys:\n","            stride = parameters[\"stride\"]\n","        else:\n","            stride = 2\n","            \n","        if \"padding\" in param_keys:\n","            padding = parameters[\"padding\"]\n","        else:\n","            padding = 0\n","        \n","        \n","        if(len(self.pre_layer_output)) != 3:\n","            raise ValueError(f\"The input shape to the Max Pooling Layer is not correct. {self.pre_layer_output}\")\n","         \n","        in_channels, in_height, in_width =  self.pre_layer_output\n","        \n","        out_height = (in_height + 2*padding - kernel_size)/ stride + 1\n","        out_width = (in_width + 2*padding - kernel_size)/ stride + 1\n","        out_channels = in_channels\n","       \n","        if out_width %1 !=0 or out_height %1 !=0 :\n","            raise ValueError(f\"Combination of pad, stride and kernel size lead to decimal number in conv layer. {out_height}\")\n","\n","\n","        self.pre_layer_output = (out_channels, int(out_height), int(out_width))\n","        \n","        max_pool_layer = (f'{self.layer}', nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=padding))\n","        \n","        return max_pool_layer\n","    \n","    def add_mobile_net(self, parameters):\n","        \"\"\"\n","        Returns a mobile net pretrained model. Currently supports V2 weights\n","        \n","        Parameters:\n","            weights: default = 'IMAGENET1K_V2'\n","            num_output_neurons = number of neurons in the last layer\n","            num_layers_to_train = number of layers to train. it starts counting from last layer. (default=0 -> only last layer is trained)\n","                                pretrained weights will be used for layers to be trained.\n","                                By passing -1, all layers will be trained. this is equal to finetuning.\n","                                \n","        \"\"\"\n","        param_keys= parameters.keys()\n","        \n","        if \"weights\" in param_keys:\n","            weights = parameters[\"weights\"]\n","        else:\n","            weights = 'IMAGENET1K_V2'\n","        \n","        if \"num_output_neurons\" in param_keys:\n","            num_output_neurons = parameters[\"num_output_neurons\"]\n","        else:\n","            raise ValueError(\"num_output_neurons must be provided while using mobile net\")\n","            \n","        if \"num_layers_to_train\" in param_keys:\n","            num_layers_to_train = parameters[\"num_layers_to_train\"]\n","        else:\n","            num_layers_to_train = 0\n","        \n","        mobile_net = models.mobilenet_v2(pretrained=True)\n","        \n","        if num_layers_to_train >=0:\n","            self.set_parameter_requires_grad(mobile_net, False)\n","            \n","            if num_layers_to_train > 0:\n","                train_features_index = len(mobile_net.features) - num_layers_to_train\n","                self.set_parameter_requires_grad(mobile_net.features[train_features_index:], True)\n","                \n","\n","        in_features = mobile_net.classifier[-1].in_features\n","        mobile_net.classifier[-1] = nn.Linear(in_features, num_output_neurons)\n","                \n","        return mobile_net\n","        \n","        \n","    def add_resnet(self, parameters):\n","        \"\"\"\n","        Returns resnet pretrained model. Currently supports ResNet18 and supports IMAGENET1K_V1\n","        \n","        Parameters:\n","            weights: default = 'IMAGENET1K_V1'\n","            num_output_neurons = number of neurons in the last layer\n","            num_layers_to_train = number of layers to train. it starts counting from last layer. (default=0 -> only last layer is trained)\n","                                pretrained weights will be used for layers to be trained.\n","                                By passing -1, all layers will be trained. this is equal to finetuning.\n","                                \n","        \"\"\"\n","        param_keys= parameters.keys()\n","        \n","        if \"weights\" in param_keys:\n","            weights = parameters[\"weights\"]\n","        else:\n","            weights = 'IMAGENET1K_V1'\n","        \n","        if \"num_output_neurons\" in param_keys:\n","            num_output_neurons = parameters[\"num_output_neurons\"]\n","        else:\n","            raise ValueError(\"num_output_neurons must be provided while using mobile net\")\n","            \n","        if \"num_layers_to_train\" in param_keys:\n","            num_layers_to_train = parameters[\"num_layers_to_train\"]\n","        else:\n","            num_layers_to_train = 0\n","            \n","        resnet = models.resnet18(pretrained=True)\n","        \n","        if num_layers_to_train >=0:\n","            self.set_parameter_requires_grad(resnet, False)\n","            \n","            if num_layers_to_train > 0:\n","                print(\"Training previous layers in ResNet18 is not supported yet\")\n","                \n","                #train_features_index = len(resnet.features) - num_layers_to_train\n","                #self.set_parameter_requires_grad(resnet.features[train_features_index:], True)\n","                \n","\n","        in_features = resnet.fc.in_features\n","        resnet.fc = nn.Linear(in_features, num_output_neurons)\n","                \n","        return resnet\n","        \n","    def add_resnet50(self, parameters):\n","        \"\"\"\n","        Returns resnet pretrained model. Currently supports ResNet18 and supports IMAGENET1K_V1\n","        \n","        Parameters:\n","            weights: default = 'IMAGENET1K_V1'\n","            num_output_neurons = number of neurons in the last layer\n","            num_layers_to_train = number of layers to train. it starts counting from last layer. (default=0 -> only last layer is trained)\n","                                pretrained weights will be used for layers to be trained.\n","                                By passing -1, all layers will be trained. this is equal to finetuning.\n","                                \n","        \"\"\"\n","        param_keys= parameters.keys()\n","        \n","        if \"weights\" in param_keys:\n","            weights = parameters[\"weights\"]\n","        else:\n","            weights = 'IMAGENET1K_V1'\n","        \n","        if \"num_output_neurons\" in param_keys:\n","            num_output_neurons = parameters[\"num_output_neurons\"]\n","        else:\n","            raise ValueError(\"num_output_neurons must be provided while using mobile net\")\n","            \n","        if \"num_layers_to_train\" in param_keys:\n","            num_layers_to_train = parameters[\"num_layers_to_train\"]\n","        else:\n","            num_layers_to_train = 0\n","            \n","        resnet = models.resnet50(pretrained=True)\n","        \n","        if num_layers_to_train >=0:\n","            self.set_parameter_requires_grad(resnet, False)\n","            \n","            if num_layers_to_train > 0:\n","                print(\"Training previous layers in ResNet50 is not supported yet\")\n","                \n","                #train_features_index = len(resnet.features) - num_layers_to_train\n","                #self.set_parameter_requires_grad(resnet.features[train_features_index:], True)\n","                \n","\n","        in_features = resnet.fc.in_features\n","        resnet.fc = nn.Linear(in_features, num_output_neurons)\n","                \n","        return resnet"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":115,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.340092Z","iopub.status.busy":"2023-01-18T05:30:31.339505Z","iopub.status.idle":"2023-01-18T05:30:31.352232Z","shell.execute_reply":"2023-01-18T05:30:31.351347Z","shell.execute_reply.started":"2023-01-18T05:30:31.340059Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["# order is important\n","#layers = OrderedDict()\n","\n","#layers[\"pretrained_model\"] = {}\n","#layers[\"pretrained_model\"][\"mobile_net\"] = {}\n","#layers[\"pretrained_model\"][\"mobile_net\"][\"name\"] = \"mobile_net\"\n","#layers[\"pretrained_model\"][\"mobile_net\"][\"num_output_neurons\"] = 4\n","#layers[\"pretrained_model\"][\"mobile_net\"][\"num_layers_to_train\"] = 5\n","\n","\n","#model = Model((5,5), layers[\"pretrained_model\"], True )\n","\n","#print(model)"]},{"cell_type":"code","execution_count":116,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.354394Z","iopub.status.busy":"2023-01-18T05:30:31.353461Z","iopub.status.idle":"2023-01-18T05:30:31.365537Z","shell.execute_reply":"2023-01-18T05:30:31.364540Z","shell.execute_reply.started":"2023-01-18T05:30:31.354358Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["#for name,param in model.named_parameters():\n","    \n","    #param.requires_grad = False\n","    #print(f\"name {name} param {param.requires_grad}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Setup loss function\n","* Binary entropy loss \n","* cross entropy loss\n","* weighted loss function\n","* maybe something from literature"]},{"cell_type":"code","execution_count":117,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.367299Z","iopub.status.busy":"2023-01-18T05:30:31.366958Z","iopub.status.idle":"2023-01-18T05:30:31.378544Z","shell.execute_reply":"2023-01-18T05:30:31.377735Z","shell.execute_reply.started":"2023-01-18T05:30:31.367266Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def setup_loss(parameters):\n","    \"\"\"\n","    Returns loss function\n","    \n","    Parameters:\n","        parameters: dict with the following keys:\n","            \"type\": \"binary\" or \"cross_entropy\"\n","            \"use_weighted_loss\": default = False\n","            \"class_weights\": weights for each class (list)\n","        \n","        Note: currently the loss function is basically Softmax + Log Loss.\n","        loss_type == \"binary\" assumes that output layer has single neuron\n","        When loss_type == \"binary\", class weights must be equal to [num_of_neg_samples/num_of_pos_samples]\n","        else: class_weights can be list containing any value but length of class_weights must be equal to number of classes\n","        \n","    \"\"\"\n","    \n","    loss_keys = parameters.keys()\n","    \n","    if \"type\" in loss_keys:\n","        loss_type = parameters[\"type\"]\n","    else:\n","        raise ValueError (\"'loss_type' must be provided\")\n","         \n","    if \"use_weighted_loss\" in loss_keys:\n","        use_weighted_loss = parameters[\"use_weighted_loss\"]\n","    else:\n","        use_weighted_loss = False\n","    \n","    if \"class_weights\" in loss_keys:\n","        class_weights = parameters[\"class_weights\"]\n","    else:\n","        class_weights = []\n","\n","    if \"device\" in loss_keys:\n","        device = parameters[\"device\"]\n","    else:\n","        device = \"cpu\"\n","\n","\n","        \n","    if use_weighted_loss and not class_weights:\n","        raise ValueError(\"When using weighted loss, class weights must be provided\")\n","\n","    class_weights = torch.tensor(class_weights)\n","    if device != \"cpu\":    \n","        class_weights = torch.tensor(class_weights).cuda()\n","    \n","    if loss_type == \"binary\":\n","        \n","        if use_weighted_loss:\n","            print(\"BC: using Weighted Loss\")\n","            loss_fnt = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n","        else:\n","            loss_fnt = nn.BCEWithLogitsLoss()\n","    \n","    elif loss_type == \"cross_entropy\":\n","         \n","        if use_weighted_loss:\n","            print(\"CE: using Weighted Loss\")\n","            loss_fnt= nn.CrossEntropyLoss(weight=class_weights)\n","        else:\n","\n","            loss_fnt= nn.CrossEntropyLoss()\n","            \n","    else:\n","        raise ValueError(\"Unknown Value for loss_type\")\n","    \n","    return loss_fnt\n","    "]},{"cell_type":"code","execution_count":118,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.381905Z","iopub.status.busy":"2023-01-18T05:30:31.381475Z","iopub.status.idle":"2023-01-18T05:30:31.391798Z","shell.execute_reply":"2023-01-18T05:30:31.390805Z","shell.execute_reply.started":"2023-01-18T05:30:31.381874Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["#loss_param = {}\n","#loss_param[\"type\"] = \"cross_entropy\"\n","#loss_param[\"use_weighted_loss\"] = True\n","#loss_param[\"class_weights\"] = [8, 6]\n","\n","#loss_fnt = setup_loss(loss_param)\n","#print(loss_fnt)"]},{"cell_type":"markdown","metadata":{},"source":["# Setup Optimizer\n","* adam\n","* SGD with momenteum\n","* something from literature\n","* regularization\n","* learning rate change with time (if required) \n","* betas"]},{"cell_type":"code","execution_count":119,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.393770Z","iopub.status.busy":"2023-01-18T05:30:31.393401Z","iopub.status.idle":"2023-01-18T05:30:31.404425Z","shell.execute_reply":"2023-01-18T05:30:31.403483Z","shell.execute_reply.started":"2023-01-18T05:30:31.393737Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def setup_optimizer(model_parameters, optim_parameters):\n","    \"\"\"\n","    Setups Optimizer\n","    \n","    Parameters:\n","        model_parameters: parameters of models that will be trained\n","        \n","        optim_parameters: dict with the following structure:\n","            optim_parameters[\"name\"] = name of optimizer (adam or sgd)\n","            optim_parameters[\"parameter1\"] = value\n","            optim_parameters[\"parameter2\"] = value\n","            \n","        for Adam optimizer, it supports following keys:\n","            \"betas\": default = (0.9,0.999)\n","            \n","        for SGD, it supports following keys.\n","            \"momentum\":  (default = 0)\n","            \"dampening\": (default=0)\n","        \n","        following keys are supported for both optimizers:\n","            \"lr\": learning rate (default = 1e-3)\n","            \"lmbda\": regularization (default = 0)     \n","            \n","    \"\"\"\n","    \n","    optim_keys=optim_parameters.keys()\n","    \n","    if \"name\" in optim_keys:\n","        optim = optim_parameters[\"name\"]\n","    else:\n","        raise ValueError(\"'name' of the optimizer must be provided\")\n","    \n","    if \"lmbda\" in optim_keys:\n","        lmbda = optim_parameters[\"lmbda\"]\n","    else:\n","        lmbda = 0\n","    \n","    if \"lr\" in optim_keys:\n","        lr = optim_parameters[\"lr\"]\n","    else: \n","        lr = 1e-3\n","    \n","    if optim == \"adam\":\n","        \n","        if \"betas\" in optim_keys:\n","            betas = optim_parameters[\"betas\"]\n","        else:\n","            betas = (0.9,0.999)\n","        \n","        optimizer = torch.optim.Adam(model_parameters,  lr=lr, betas=betas, amsgrad=True, weight_decay=lmbda)\n","    \n","    elif optim == \"sgd\":\n","        \n","        if \"momentum\" in optim_keys:\n","            momentum = optim_parameters[\"momentum\"]\n","        else:\n","            momentum = 0\n","        \n","        if \"dampening\" in optim_keys:\n","            dampening = optim_parameters[\"dampening\"]\n","        else:\n","            dampening = 0\n","        \n","        optimizer = torch.optim.SGD(model_parameters, momentum=momentum, dampening=dampening, weight_decay=lmbda)\n","    else:\n","        raise ValueError(\"Unknown name of the optimizer provided\")\n","    \n","    return optimizer"]},{"cell_type":"code","execution_count":120,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.406427Z","iopub.status.busy":"2023-01-18T05:30:31.405853Z","iopub.status.idle":"2023-01-18T05:30:31.417958Z","shell.execute_reply":"2023-01-18T05:30:31.417013Z","shell.execute_reply.started":"2023-01-18T05:30:31.406390Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["#optim_parameters = {}\n","#optim_parameters[\"name\"] = \"adam\"\n","#optimizer = setup_optimizer (model.parameters(), optim_parameters)\n","#print(optimizer)"]},{"cell_type":"markdown","metadata":{},"source":["# Metrics"]},{"cell_type":"markdown","metadata":{},"source":["## Accuracy "]},{"cell_type":"code","execution_count":121,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.420154Z","iopub.status.busy":"2023-01-18T05:30:31.419483Z","iopub.status.idle":"2023-01-18T05:30:31.432872Z","shell.execute_reply":"2023-01-18T05:30:31.430887Z","shell.execute_reply.started":"2023-01-18T05:30:31.420120Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def accuracy(y_true, y_pred, parameters):\n","    \"\"\"\n","    Calculates accuracy\n","    \n","    Parameters:\n","        y_true: numpy array of shape (num_images,)\n","        y_pred: numpy array of shape (num_images,)\n","        parameters: dictionary with the following keys:\n","            type: \"simple\" (default) or \"balanced\"\n","    \n","    Returns:\n","        score: float\n","        config:\n","\n","    Additional Notes:\n","\n","        Simple Accuracy: fraction of labels that are equal.\n","        Balanced Accuracy: \n","            Binary class: equal to the arithmetic mean of sensitivity (true positive rate) \n","                            and specificity (true negative rate)\n","                             = 1/2 ( (TP/TP+FN) + (TN/TN+FP))\n","\n","            Multiclass: the macro-average of recall scores per class\n","                        recall for each class and then take the mean\n","                        recall = TP /(TP+FN)\n","\n","            if the classifier predicts same label for all examples, the score will be equal to 1/num_classes (for binary: 0.5)\n","            for more info, see\n","                \"https://scikit-learn.org/stable/modules/model_evaluation.html#balanced-accuracy-score\"\n","    \"\"\"\n","\n","    config={}\n","    if \"type\" in parameters.keys():\n","        accu_type = parameters[\"type\"]\n","    else:\n","        accu_type = \"simple\"\n","\n","    config[\"type\"] = accu_type\n","    \n","    if accu_type == \"simple\":\n","        score = accuracy_score(y_true=y_true, y_pred=y_pred) \n","    elif accu_type == \"balanced\":\n","        score = balanced_accuracy_score(y_true=y_true, y_pred=y_pred)\n","    else:\n","        raise ValueError(\"Unknown Value encountered for parameter 'type' while calculating accuracy\")#\n","\n","    \n","    return score, config"]},{"cell_type":"code","execution_count":122,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.436190Z","iopub.status.busy":"2023-01-18T05:30:31.435323Z","iopub.status.idle":"2023-01-18T05:30:31.447692Z","shell.execute_reply":"2023-01-18T05:30:31.446603Z","shell.execute_reply.started":"2023-01-18T05:30:31.436148Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["#y_true = np.array([0, 2, 1, 3])\n","#y_predict = np.array([0, 1, 2, 3])\n","#print(\"simple: \", accuracy(y_true, y_predict, {}))\n","#y_true = np.array([0, 1, 0, 0, 1, 0])\n","#y_predict = np.array([0, 1, 0, 0, 0, 1])\n","#print(\"balanced: \", accuracy(y_true, y_predict, {\"type\": \"balanced\"}))"]},{"cell_type":"markdown","metadata":{},"source":["## MCC "]},{"cell_type":"code","execution_count":123,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.450414Z","iopub.status.busy":"2023-01-18T05:30:31.449617Z","iopub.status.idle":"2023-01-18T05:30:31.463715Z","shell.execute_reply":"2023-01-18T05:30:31.462644Z","shell.execute_reply.started":"2023-01-18T05:30:31.450245Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def mcc(y_true, y_pred, parameters):\n","    \"\"\"\n","    Calculates mcc\n","    \n","    Parameters:\n","        y_true: numpy array of shape (num_images,)\n","        y_pred: numpy array of shape (num_images,)\n","        parameters: dictionary with the following keys:\n","            \n","    Returns:\n","        score: float\n","        config:\n","\n","    Additional Notes:\n","        Binary:\n","        \n","            +1 -> prefect, 0-> random, -1 -> inverse \n","\n","            mcc = (tp*tn) - (fp*fn) / sqrt( (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)  )\n","\n","        Multiclass:\n","            \n","            +1 -> perfect, between -1 and 0 -> min\n","\n","            for more info, see\n","                \"https://scikit-learn.org/stable/modules/model_evaluation.html#matthews-corrcoef\"\n","    \"\"\"\n","\n","    config = {}\n","\n","    score = matthews_corrcoef(y_true=y_true, y_pred=y_pred)\n","\n","    return score, config"]},{"cell_type":"code","execution_count":124,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.465880Z","iopub.status.busy":"2023-01-18T05:30:31.465225Z","iopub.status.idle":"2023-01-18T05:30:31.475804Z","shell.execute_reply":"2023-01-18T05:30:31.474717Z","shell.execute_reply.started":"2023-01-18T05:30:31.465821Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["#y_true = np.array([+1, +1, +1, -1])\n","#y_predict = np.array([+1, -1, +1, +1])\n","#print(\"mcc: \", mcc(y_true, y_predict, {}))"]},{"cell_type":"markdown","metadata":{},"source":["## Sensitivity "]},{"cell_type":"code","execution_count":125,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.478270Z","iopub.status.busy":"2023-01-18T05:30:31.477435Z","iopub.status.idle":"2023-01-18T05:30:31.498055Z","shell.execute_reply":"2023-01-18T05:30:31.495724Z","shell.execute_reply.started":"2023-01-18T05:30:31.478227Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def sensitivity(y_true, y_pred, parameters):\n","    \"\"\"\n","    Calculates sensitivity (recall)\n","    \n","    Parameters:\n","        y_true: numpy array of shape (num_images,)\n","        y_pred: numpy array of shape (num_images,)\n","        parameters: dictionary with the following keys:\n","            class_result: for binary classes, name of class for which metric will be calculated\n","            average: for mulitlcass, how to calculate metrics for each class: 'micro', 'macro', 'weighted' \n","    \n","    Returns:\n","        score: float\n","        config:\n","\n","    Additional Notes:\n","        recall = TP/ TP+FN\n","\n","        average:\n","            'micro': Calculate metrics globally by counting the total true positives, false negatives and false positives\n","            'macro': Calculate metrics for each label, and find their unweighted mean\n","            'weighted': Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label)\n","\n","    \"\"\"\n","\n","    config = {}\n","\n","    if len(np.unique(y_true)) <= 2 and len(np.unique(y_pred)) <= 2:\n","\n","        if \"class_result\" in parameters.keys():\n","            pos_label = parameters[\"class_result\"]\n","        else:\n","           ytl = np.unique(y_true)\n","           ypl = np.unique(y_pred)\n","           labels = np.unique(np.concatenate((ytl, ypl))) \n","           pos_label = labels[0]\n","        \n","        config[\"class_result\"] = pos_label\n","\n","        score = recall_score(y_true=y_true, y_pred=y_pred, pos_label=pos_label)\n","    \n","    else:\n","        \n","        if \"average\" in parameters.keys():\n","            average = parameters[\"average\"]\n","        else:\n","            average = \"weighted\"\n","\n","        config[\"average\"] = average\n","\n","        score = recall_score(y_true=y_true, y_pred=y_pred, average=average)\n","\n","    return score, config"]},{"cell_type":"markdown","metadata":{},"source":["## Precision "]},{"cell_type":"code","execution_count":126,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.500508Z","iopub.status.busy":"2023-01-18T05:30:31.499714Z","iopub.status.idle":"2023-01-18T05:30:31.516683Z","shell.execute_reply":"2023-01-18T05:30:31.515783Z","shell.execute_reply.started":"2023-01-18T05:30:31.500468Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def precision(y_true, y_pred, parameters):\n","    \"\"\"\n","    Calculates precision\n","    \n","    Parameters:\n","        y_true: numpy array of shape (num_images,)\n","        y_pred: numpy array of shape (num_images,)\n","        parameters: dictionary with the following keys:\n","            class_result: for binary classes, name of class for which metric will be calculated\n","            average: for mulitlcass, how to calculate metrics for each class: 'micro', 'macro', 'weighted' \n","    \n","    Returns:\n","        score: float\n","        config:\n","\n","    Additional Notes:\n","        precision = TP/ TP+FP\n","\n","        average:\n","            'micro': Calculate metrics globally by counting the total true positives, false negatives and false positives\n","            'macro': Calculate metrics for each label, and find their unweighted mean\n","            'weighted': Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label)\n","\n","    \"\"\"\n","    config = {}\n","\n","    if len(np.unique(y_true)) <= 2 and len(np.unique(y_pred)) <= 2:\n","\n","        if \"class_result\" in parameters.keys():\n","            pos_label = parameters[\"class_result\"]\n","        else:\n","           ytl = np.unique(y_true)\n","           ypl = np.unique(y_pred)\n","           labels = np.unique(np.concatenate((ytl, ypl))) \n","           pos_label = labels[0]\n","        \n","        config[\"class_result\"] = pos_label\n","\n","        score = precision_score(y_true=y_true, y_pred=y_pred, pos_label=pos_label)\n","    \n","    else:\n","        \n","        if \"average\" in parameters.keys():\n","            average = parameters[\"average\"]\n","        else:\n","            average = \"weighted\"\n","\n","        config[\"average\"] = average\n","\n","        score = precision_score(y_true=y_true, y_pred=y_pred, average=average)\n","\n","    return score, config"]},{"cell_type":"markdown","metadata":{},"source":["## F1 Score"]},{"cell_type":"code","execution_count":127,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.520030Z","iopub.status.busy":"2023-01-18T05:30:31.519737Z","iopub.status.idle":"2023-01-18T05:30:31.531184Z","shell.execute_reply":"2023-01-18T05:30:31.530448Z","shell.execute_reply.started":"2023-01-18T05:30:31.520006Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def F1_score(y_true, y_pred, parameters):\n","    \"\"\"\n","    Calculates f1 score\n","    \n","    Parameters:\n","        y_true: numpy array of shape (num_images,)\n","        y_pred: numpy array of shape (num_images,)\n","        parameters: dictionary with the following keys:\n","            class_result: for binary classes, name of class for which metric will be calculated\n","            average: for mulitlcass, how to calculate metrics for each class: 'micro', 'macro', 'weighted' \n","    \n","    Returns:\n","        score: float\n","        config:\n","\n","    Additional Notes:\n","        f1_score = 2 * (Precision * Recall)/ (Precision + Recall)\n","\n","        average:\n","            'micro': Calculate metrics globally by counting the total true positives, false negatives and false positives\n","            'macro': Calculate metrics for each label, and find their unweighted mean\n","            'weighted': Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label)\n","\n","    \"\"\"\n","\n","    config = {}\n","\n","    if len(np.unique(y_true)) <= 2 and len(np.unique(y_pred)) <= 2:\n","\n","        if \"class_result\" in parameters.keys():\n","            pos_label = parameters[\"class_result\"]\n","        else:\n","           ytl = np.unique(y_true)\n","           ypl = np.unique(y_pred)\n","           labels = np.unique(np.concatenate((ytl, ypl))) \n","           pos_label = labels[0]\n","        \n","        config[\"class_result\"] = pos_label\n","\n","        score = f1_score(y_true=y_true, y_pred=y_pred, pos_label=pos_label)\n","    \n","    else:\n","        \n","        if \"average\" in parameters.keys():\n","            average = parameters[\"average\"]\n","        else:\n","            average = \"weighted\"\n","\n","        config[\"average\"] = average\n","\n","        score = f1_score(y_true=y_true, y_pred=y_pred, average=average)\n","\n","    return score, config\n"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluate Metrics "]},{"cell_type":"code","execution_count":128,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.537736Z","iopub.status.busy":"2023-01-18T05:30:31.536817Z","iopub.status.idle":"2023-01-18T05:30:31.549527Z","shell.execute_reply":"2023-01-18T05:30:31.548872Z","shell.execute_reply.started":"2023-01-18T05:30:31.537701Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def evaluate_metrics(y_true, y_pred, metrics, y_pred_probs=None):\n","    \"\"\"\n","    Applies each metric and generates evaluation score\n","\n","    Parameters:\n","        y_true: numpy array of shape (folds, num_images, 2)\n","        y_pred: numpy array of shape (folds, num_images, 2)\n","        metrics: dictionary with following structure:\n","            metrics[\"metrics_1\"][\"name\"] = name of metric\n","            metrics[\"metrics_1\"][\"parameter_1\"] = value\n","            metrics[\"metrics_1\"][\"parameter_2\"] = value\n","\n","            metrics[\"metrics_2\"][\"name\"] = name of metric\n","            metrics[\"metrics_2\"][\"parameter_1\"] = value\n","            metrics[\"metrics_2\"][\"parameter_2\"] = value\n","            \n","    \n","    currently, supports \"accuracy\", \"mcc\", \"precision\", \"sensitivity\", \"F1_score\"\n","  \n","\n","    Returns:\n","        scores:numpy array of shape (folds, metrics)\n","        output_config:\n","       \n","    Additional Notes:\n","\n","    \"\"\"\n","    # check whether correct shapes of y_* are provided or not\n","\n","    if len(y_true.shape) !=3:\n","        raise ValueError(\"Shape of y_true is not correct.\")\n","\n","    if len(y_pred.shape) == 3:\n","        num_folds = y_pred.shape[0]\n","        met_keys = list(metrics.keys())\n","        num_metrics = len(met_keys)\n","    else:\n","        raise ValueError(\"Shape of y_pred is not correct.\")\n","\n","    config={}\n","\n","    list_of_metrics = []\n","\n","    scores = np.full((num_folds, num_metrics), 1000, dtype=np.float32)\n","    \n","    flag = True\n","\n","\n","    for fold_no in range(num_folds):\n","\n","        for metric_no in range(num_metrics):\n","\n","            print(f\"Processing: Fold No: {fold_no} Metric: {met_keys[metric_no]}\")\n","\n","            metric_name = metrics[met_keys[metric_no]][\"name\"]\n","            \n","            if metric_name == \"accuracy\":\n","                fnt_pointer = accuracy\n","            elif metric_name == \"mcc\":\n","                fnt_pointer = mcc\n","            elif metric_name == \"precision\":\n","                fnt_pointer = precision\n","            elif metric_name ==\"sensitivity\":\n","                fnt_pointer = sensitivity\n","            elif metric_name ==\"F1_score\":\n","                fnt_pointer = F1_score\n","            else:\n","                raise ValueError(\"Unknown metric found\")\n","\n","            \n","            metric_score, fnt_config = fnt_pointer(y_true=y_true[fold_no, :, 1], \n","                                                    y_pred=y_pred[fold_no, :, 1], \n","                                                    parameters=metrics[met_keys[metric_no]])\n","\n","\n","            scores[fold_no, metric_no] = metric_score\n","\n","            # setup output config\n","            if flag:\n","                fnt_config[\"name\"] = metric_name\n","                config[met_keys[metric_no]] = {} \n","                config[met_keys[metric_no]] = fnt_config\n","\n","                list_of_metrics.append(met_keys[metric_no])\n","\n","        flag=False\n","    \n","    return scores, config, list_of_metrics\n"]},{"cell_type":"code","execution_count":129,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.551561Z","iopub.status.busy":"2023-01-18T05:30:31.550793Z","iopub.status.idle":"2023-01-18T05:30:31.564074Z","shell.execute_reply":"2023-01-18T05:30:31.563238Z","shell.execute_reply.started":"2023-01-18T05:30:31.551528Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["# multi\n","#y_true = np.array([\"aa\", \"a\", \"ab\", \"b\", \"ac\", \"c\",\n","#                    \"ad\", \"a\", \"ae\", \"c\", \"af\", \"a\",\n","#                    \"ag\", \"c\", \"ah\", \"b\", \"ai\", \"a\"]).reshape(3,3, 2)\n","#y_pred = np.array([\"aa\", \"b\", \"ab\", \"b\", \"ac\", \"b\",\n","#                    \"ad\", \"a\", \"ae\", \"c\", \"af\", \"a\",\n","#                    \"ag\", \"a\", \"ah\", \"b\", \"ai\", \"c\",]).reshape(3,3,2)\n","\n","\n","\n","\n","#metrics = {}\n","\n","#metrics[\"simple_accuracy\"] = {}\n","#metrics[\"simple_accuracy\"][\"name\"] = \"accuracy\"\n","#metrics[\"simple_accuracy\"][\"type\"] = \"simple\"  \n","\n","#metrics[\"balanced_accuracy\"] = {}\n","#metrics[\"balanced_accuracy\"][\"name\"] = \"accuracy\"\n","#metrics[\"balanced_accuracy\"][\"type\"] = \"balanced\"\n","\n","\n","# precision\n","#metrics[\"precision\"] = {}\n","#metrics[\"precision\"][\"name\"] = \"precision\" \n","#metrics[\"precision\"][\"class_result\"] = \"a\"\n","#metrics[\"precision\"][\"average\"] = \"weighted\"\n","\n","\n","# recall\n","#metrics[\"sensitivity\"] = {}\n","#metrics[\"sensitivity\"][\"name\"] = \"sensitivity\"\n","#metrics[\"sensitivity\"][\"class_result\"]  = \"a\"\n","#metrics[\"sensitivity\"][\"average\"]  = \"weighted\"\n","\n","# f1_score\n","#metrics[\"f1_score\"] = {}\n","#metrics[\"f1_score\"][\"name\"] = \"F1_score\" \n","#metrics[\"f1_score\"][\"class_result\"] = \"a\"\n","#metrics[\"f1_score\"][\"average\"] = \"weighted\"\n","\n","# mcc\n","#metrics[\"mcc\"] = {}\n","#metrics[\"mcc\"][\"name\"] = \"mcc\" \n","\n","\n","#print(\"true labels\")\n","#print(y_true)\n","#print(y_true.shape)\n","\n","#print(\"pred labels\")\n","#print(y_pred)\n","#print(y_pred.shape)\n","\n","#scores, config, list_of_metrics =evaluate_metrics(y_true=y_true, y_pred=y_pred, metrics =metrics)\n","\n","#print(\"scores \")\n","#print(scores)\n","\n","#print(\"config \")\n","#print(config)\n","\n","#print(\"metrics list \")\n","#print(list_of_metrics)"]},{"cell_type":"markdown","metadata":{},"source":["# Plots"]},{"cell_type":"markdown","metadata":{},"source":["## Learning Curves "]},{"cell_type":"code","execution_count":130,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.566495Z","iopub.status.busy":"2023-01-18T05:30:31.565388Z","iopub.status.idle":"2023-01-18T05:30:31.578959Z","shell.execute_reply":"2023-01-18T05:30:31.578054Z","shell.execute_reply.started":"2023-01-18T05:30:31.566462Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def plot_LC(metric_score, path_to_results, path_to_images):\n","\n","    \"\"\"\n","    Plots and saves learning curves\n","    \n","    metric_score: numpy array of shape (metrics, 2, epochs)\n","    path_to_results: path where plot will be saved\n","    \n","    \"\"\"\n","    num_metrics = metric_score.shape[0]\n","    metrics = [\"CELoss\", \"Bal_Accu\", \"MCC\"]\n","    \n","    fig, axes = plt.subplots(num_metrics, 1)\n","    \n","    fig.suptitle(\"Learning Curves\")\n","\n","    for met in range(num_metrics):\n","\n","        # plot training curve\n","        train_line, = axes[met].plot(metric_score[met][0], color='blue', label='Training')\n","\n","        # plot validation curve\n","        valid_line, = axes[met].plot(metric_score[met][1], color= 'orangered', label='Validation')\n","\n","        # setup x-axis\n","        if met != num_metrics -1 :\n","            axes[met].set_xticks([])\n","        else:\n","            axes[met].set_xlabel(\"Epochs\")\n","\n","        # setup y-axis\n","        axes[met].set_ylabel(metrics[met])\n","\n","    fig.legend(handles= [train_line, valid_line])\n","\n","    plt.savefig(path_to_results + \"_LC\")\n","    plt.close()"]},{"cell_type":"code","execution_count":131,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.581178Z","iopub.status.busy":"2023-01-18T05:30:31.580139Z","iopub.status.idle":"2023-01-18T05:30:31.593864Z","shell.execute_reply":"2023-01-18T05:30:31.592854Z","shell.execute_reply.started":"2023-01-18T05:30:31.581144Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["#x= np.random.rand(3, 2, 10)\n","#x[0,0] = np.arange(1, 11)\n","#x[1, 1] = np.arange(11,21)\n","\n","#print(x)\n","#path_to_results = \"/\"\n","\n","#plot_LC(\n","#    x, path_to_results, x)"]},{"cell_type":"markdown","metadata":{},"source":["## Misidentified Samples "]},{"cell_type":"code","execution_count":132,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.596251Z","iopub.status.busy":"2023-01-18T05:30:31.595673Z","iopub.status.idle":"2023-01-18T05:30:31.618708Z","shell.execute_reply":"2023-01-18T05:30:31.617725Z","shell.execute_reply.started":"2023-01-18T05:30:31.596148Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def plot_MS(y_true, y_pred, path_to_results, path_to_images):\n","    \"\"\"\n","    Plots and save missclassified samples\n","    \n","    y_true: numpy array of shape (num_of_images,2)\n","    y_pred: numpy array of shape (num_of_images,2)\n","    path_to_results: path where plot will be saved\n","    path_to_images: folder containing images\n","\n","\n","    Only works with (num_samples)^2 = whole number\n","\n","    \"\"\"\n","    \n","    num_samples_to_plot = 4\n","\n","    classes = np.unique(y_true[:,1])\n","\n","    if np.sqrt(num_samples_to_plot) %1 !=0:\n","        raise ValueError(\"Currently, this function only supports those number of samples whose sqaure is a whole number\")\n","\n","    \n","    \n","    cm = create_dict(classes)\n","\n","    for true_label in classes:\n","        \n","        # get all images for a class in y_true\n","        pos = y_true[np.where(y_true[:,1] == true_label)]\n","        \n","        pred_classes = classes.tolist()\n","      \n","        for sample in pos:\n","            \n","            # find given image in y_pred\n","            img_id = y_pred[np.where(y_pred[:,0] == sample[0])]\n","            \n","            # store img_ids in their respective col\n","            for pred_label in pred_classes:\n","                if img_id[0,1] == pred_label:\n","                    cm[true_label][pred_label].append(img_id[0,0])\n","\n","            # check whether there are requried number of samples in each col\n","            for l in cm[true_label].keys():\n","                \n","                if l in pred_classes and len(cm[true_label][l]) == num_samples_to_plot:\n","                    pred_classes.remove(l)\n","                \n","            if not len(pred_classes):\n","                break\n","    \n","    plot_CM_images(cm, num_samples_to_plot, path_to_images, path_to_results)\n","\n","\n","\n","def plot_CM_images(cm, num_samples, path_to_images, path_to_results):\n","    \"\"\"\n","    Plots and saves images\n","    \n","    \"\"\"\n","    num_classes = len(cm.keys())\n","    \n","    num_imgs_axis = int(np.sqrt(num_samples)) \n","    \n","    num_rows= num_imgs_axis * num_classes\n","    num_cols = num_imgs_axis * num_classes\n","\n","    fig, axes = plt.subplots(num_rows, num_cols, figsize=(10,10))\n","    \n","    fig.suptitle(\"Confusion Matrix of misclassified images\")\n","    plt.subplots_adjust(wspace=0, hspace=0)\n","\n","    fig.text(0.5, 0.04, 'Predicted Labels', ha='center', va='center')\n","    fig.text(0.06, 0.5, 'True Labels', ha='center', va='center', rotation='vertical')\n","\n","    for i, true_label in enumerate(cm.keys()):\n","        \n","        pred_labels = cm[true_label]\n","        \n","        if i==0:\n","            row_i = i\n","        \n","\n","        for j, pred_label in enumerate(pred_labels.keys()):\n","\n","            img_ids = pred_labels[pred_label]\n","\n","            empty_img_ids = num_samples - len(img_ids)\n","\n","            if j==0:\n","                col_j = j\n","            \n","            row = 0\n","            col = 0\n","\n","            for id in img_ids:\n","\n","                img = mpimg.imread(os.path.join(path_to_images, id))\n","                img_shape = img.shape\n","                \n","                axes[row_i+row, col_j+col].imshow(img, cmap='gray', vmin=0, vmax=255, aspect='auto')\n","                axes[row_i+row, col_j+col].set_xticks([])\n","                axes[row_i+row, col_j+col].set_yticks([])\n","\n","                if row_i + row == num_rows-1:\n","                    axes[row_i+row, col_j+col].set_xlabel(pred_label)\n","\n","            \n","                if col_j + col == 0:\n","                    axes[row_i+row, col_j+col].set_ylabel(true_label)\n","\n","\n","\n","\n","                if col ==num_imgs_axis-1:\n","                    col =0\n","                    row +=1\n","                else:\n","                    col +=1\n","\n","            if len(img_ids) ==0:\n","                img_shape = (299,299)        \n","            \n","            if empty_img_ids > 0:\n","                temp = np.full(img_shape, 255)\n","\n","                for _ in range(empty_img_ids):\n","                    \n","                    axes[row_i+row, col_j+col].imshow(temp, cmap='gray', vmin=0, vmax=255, aspect='auto')\n","                    axes[row_i+row, col_j+col].set_xticks([])\n","                    axes[row_i+row, col_j+col].set_yticks([])\n","\n","                    if row_i + row == num_rows-1:\n","                        axes[row_i+row, col_j+col].set_xlabel(pred_label)\n","\n","                \n","                    if col_j + col == 0:\n","                        axes[row_i+row, col_j+col].set_ylabel(true_label)\n","                        \n","                    \n","                    if col ==num_imgs_axis-1:\n","                        col =0\n","                        row+=1\n","                    else:\n","                        col +=1\n","                   \n","            if empty_img_ids<0:\n","                raise ValueError(\"There are more image ids in the dict than number of samples to be plotted\")\n","            \n","            col_j = col_j + num_imgs_axis\n","        \n","        row_i += num_imgs_axis\n","    \n","    plt.savefig(path_to_results + \"_MS\")\n","    plt.close()\n","\n","    \n","def create_dict(classes):\n","    \"\"\"\n","    Setups a dictionary for storing image ids in confusion matrix\n","    \n","    \"\"\"\n","\n","    output_dict = dict()\n","    for i in classes:\n","        output_dict[i] = {}\n","        for j in classes:\n","            output_dict[i][j]= []\n","\n","    return output_dict      "]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true}},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## Confusion Matrix"]},{"cell_type":"code","execution_count":133,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.621078Z","iopub.status.busy":"2023-01-18T05:30:31.620017Z","iopub.status.idle":"2023-01-18T05:30:31.633677Z","shell.execute_reply":"2023-01-18T05:30:31.632673Z","shell.execute_reply.started":"2023-01-18T05:30:31.621045Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def plot_CM(y_true, y_pred, path_to_results, path_to_images):\n","    \"\"\"\n","    Plots and save Confusion Matrix\n","    \n","    y_true: numpy array of shape (num_of_images,2)\n","    y_pred: numpy array of shape (num_of_images,2)\n","    path_to_results: path where plot will be saved\n","\n","    \"\"\"\n","\n","    ConfusionMatrixDisplay.from_predictions(y_pred=y_pred[:,1], y_true =y_true[:,1])\n","\n","    plt.savefig(path_to_results + \"_CM\")\n","    plt.close()"]},{"cell_type":"markdown","metadata":{},"source":["## Create Plots "]},{"cell_type":"code","execution_count":134,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.635857Z","iopub.status.busy":"2023-01-18T05:30:31.635108Z","iopub.status.idle":"2023-01-18T05:30:31.648567Z","shell.execute_reply":"2023-01-18T05:30:31.647601Z","shell.execute_reply.started":"2023-01-18T05:30:31.635798Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def create_plots(y_true, y_pred, path_to_results, path_to_images, plots, name_of_file, training_metric_scores=None, y_pred_probs = None):\n","    \"\"\"\n","    Generates differenet plots\n","    Parameters:\n","        y_true: numpy array of shape (folds, num_images, 2)\n","        y_pred: numoy array of shape (folds, num_images, 2)\n","        path_to_results: path to the folder where the results will be stored\n","        plots: dictionary with following structure:\n","                plots[\"plots_1\"][\"name\"] = name of plot\n","                plots[\"plots_1\"][\"parameter_1\"] = value\n","                plots[\"plots_1\"][\"parameter_2\"] = value\n","\n","                plots[\"plots_2\"][\"name\"] = name of plot\n","                plots[\"plots_2\"][\"parameter_1\"] = value\n","                plots[\"plots_2\"][\"parameter_2\"] = value\n","        \n","        name_of_file:\n","        training_metric_scores: (default=None) list of numpy array with each having shape of (folds, metrics, 2, epochs).Each item in list should represent results of a network.\n","\n","    Returns:\n","        output_config:\n","\n","    Additional Notes:\n","    currently supports CM, LC and MS\n","\n","\n","    \"\"\"\n","\n","    # check whether correct shapes of y_* are provided or not\n","\n","    if len(y_true.shape) !=3:\n","        raise ValueError(\"Shape of y_true is not correct.\")\n","\n","    if len(y_pred.shape) == 3:\n","        num_folds = y_pred.shape[0]\n","    else:\n","        raise ValueError(\"Shape of y_pred is not correct.\")\n","\n","    # determine whether to plot learning curves or not\n","    if training_metric_scores is not None:\n","        lc = True\n","    else:\n","        lc = False\n","   \n","\n","\n","    for fold_no in range(num_folds):\n","\n","        # create directory for fold\n","        path_to_fold = os.path.join(path_to_results, str(fold_no))\n","        if not os.path.exists(path_to_fold):\n","            os.mkdir(path_to_fold)\n","\n","        # create directory for plots\n","        path_to_plots = os.path.join(path_to_fold, \"plots\")\n","        if not os.path.exists(path_to_plots):\n","            os.mkdir(path_to_plots)\n","\n","        for pl in plots:\n","\n","            # if dict has key \"learning_curves\" but no metric score is provided,\n","            if pl == \"LC\" and not lc:\n","                print(\"Warning: key'learning_curves' provided in dict 'plots' but metric score not found. Skipping ploting learning curves\" )\n","                continue \n","\n","\n","            print(f\"Creating Plot: {pl} Fold No: {fold_no}\")\n","            \n","            plot_name = pl\n","            if plot_name == \"CM\":\n","                fnt_pointer = plot_CM\n","            elif plot_name == \"MS\":\n","                fnt_pointer = plot_MS\n","            elif plot_name == \"LC\":\n","                fnt_pointer = plot_LC\n","            else:\n","                raise ValueError(\"Unknown plot found\")\n","                \n","            path_to_figs = path_to_plots+f\"/{name_of_file}\"\n","\n","            if plot_name == \"LC\":\n","                fnt_pointer(metric_score= training_metric_scores[fold_no], path_to_results=path_to_figs, path_to_images=path_to_images)\n","            else:\n","                fnt_pointer(y_true=y_true[fold_no], y_pred=y_pred[fold_no], path_to_results=path_to_figs, path_to_images=path_to_images)\n","\n","\n","    return plots"]},{"cell_type":"markdown","metadata":{},"source":["# Misc"]},{"cell_type":"markdown","metadata":{},"source":["## Load from json "]},{"cell_type":"code","execution_count":135,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.650918Z","iopub.status.busy":"2023-01-18T05:30:31.649958Z","iopub.status.idle":"2023-01-18T05:30:31.661703Z","shell.execute_reply":"2023-01-18T05:30:31.660719Z","shell.execute_reply.started":"2023-01-18T05:30:31.650874Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def load_from_json(path_to_json):\n","    \"\"\"\n","    Load a json file\n","\n","    Parameters:\n","        path_to_json: path to json file\n","    \n","    Returns:\n","        dictionary with contents of the json\n","    \"\"\"\n","    with open(path_to_json, 'r') as f:\n","        dic = json.load(f)\n","    \n","    return dic"]},{"cell_type":"markdown","metadata":{},"source":["## Save to json "]},{"cell_type":"code","execution_count":136,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.663921Z","iopub.status.busy":"2023-01-18T05:30:31.663073Z","iopub.status.idle":"2023-01-18T05:30:31.672180Z","shell.execute_reply":"2023-01-18T05:30:31.671212Z","shell.execute_reply.started":"2023-01-18T05:30:31.663877Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def save_to_json(dic, path_to_results):\n","    \"\"\"\n","    Save a dicitonary to a location\n","\n","    Parameters:\n","        dic: dictionary to be saved\n","        path_to_results: path where to save the dictionary\n","\n","    \"\"\"\n","    with open(path_to_results+\".json\", \"w\") as fp:\n","        json.dump(dic, fp, indent=4)"]},{"cell_type":"markdown","metadata":{},"source":["## Generate txt file "]},{"cell_type":"code","execution_count":137,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.676780Z","iopub.status.busy":"2023-01-18T05:30:31.676108Z","iopub.status.idle":"2023-01-18T05:30:31.685774Z","shell.execute_reply":"2023-01-18T05:30:31.685076Z","shell.execute_reply.started":"2023-01-18T05:30:31.676741Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def generate_txt_file(y, path_to_results, name_of_file, y_probs=None):\n","    \"\"\"\n","    Generates a text file with structure as follows:\n","    file_name_1 label\n","    file_name_2 label\n","\n","\n","    Parameters:\n","        y: numpy array of shape(folds, num_images, 2)\n","        path_to_results: path to the folder where to store results\n","        name_of_file (without .txt)\n","        y_probs: numpy array of shape (folds, num_images, 1)\n","\n","    \"\"\"\n","    \n","\n","    num_folds = y.shape[0]\n","    num_images = y.shape[1]\n","\n","\n","    for fold_no in range(num_folds):\n","\n","        print(f\"Processing Fold No: {fold_no}\")\n","\n","        path_to_fold = os.path.join(path_to_results, str(fold_no))\n","        if not os.path.exists(path_to_fold):\n","                os.mkdir(path_to_fold)\n","        \n","        # generate file\n","        path_to_file = path_to_fold + \"/\" + name_of_file + \".txt\"\n","        open(path_to_file, \"w\").close()\n","        with open(path_to_file, \"a\") as file:\n","            for img_no in range(num_images):\n","                file.write(f\"{y[fold_no, img_no, 0]} {y[fold_no, img_no, 1]}\\n\")\n","        \n","        # generate files with prob\n","        if y_probs is not None:\n","            path_to_prob_file = path_to_fold + \"/\" + name_of_file + \"_with_probs.txt\"\n","            open(path_to_prob_file, \"w\").close()\n","            with open(path_to_prob_file, \"a\") as file:\n","                for img_no in range(num_images):\n","                    file.write(f\"{y[fold_no, img_no, 0]} {y[fold_no, img_no, 1]} {np.round(y_probs[fold_no, img_no], 4)*100}\\n\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Save results "]},{"cell_type":"code","execution_count":138,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.687622Z","iopub.status.busy":"2023-01-18T05:30:31.686966Z","iopub.status.idle":"2023-01-18T05:30:31.699793Z","shell.execute_reply":"2023-01-18T05:30:31.699108Z","shell.execute_reply.started":"2023-01-18T05:30:31.687585Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def save_results(results, metrics, path_to_results, name_of_file):\n","    \"\"\"\n","    Save results to csv file\n","\n","    Parameters:\n","        results: numpy array of shape (num_folds, metrics)\n","        metrics: numpy array of name of the metrics\n","        path_to_results: path to the folder where the results will be stored\n","        name_of_file: file name\n","       \n","    \"\"\"\n","    df = pd.DataFrame([], columns=[\"Fold No\", \"Metric\", \"Score\"])\n","    num_folds = results.shape[0]\n","    num_metrics = results.shape[1]\n","\n","    for fold_no in range(num_folds):\n","        for metric_no in range(num_metrics):\n","            temp = pd.DataFrame([[fold_no, metrics[metric_no], results[fold_no, metric_no]]], columns=[\"Fold No\", \"Metric\", \"Score\"])\n","            df = pd.concat([df, temp])\n","\n","    df.reset_index(inplace=True, drop=True ) \n","       \n","    df.to_csv(path_to_results + \"/\" + name_of_file +\".csv\")"]},{"cell_type":"markdown","metadata":{},"source":["## Count Model parameters "]},{"cell_type":"code","execution_count":139,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.750659Z","iopub.status.busy":"2023-01-18T05:30:31.749779Z","iopub.status.idle":"2023-01-18T05:30:31.758021Z","shell.execute_reply":"2023-01-18T05:30:31.756975Z","shell.execute_reply.started":"2023-01-18T05:30:31.750621Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def count_parameters(model):\n","    \"\"\"\n","    \n","    Counts number of parameters in the model\n","    \n","    Parameter:\n","        model\n","    \n","    Returns:\n","        a dict with keys:\n","            parameters: \n","                name: dict with keys: trainable and total \n","            total_trainable_parameters:\n","            total_parameters\n","    \n","    \"\"\"\n","    output_config = {}\n","    output_config[\"parameters\"] = {}\n","    total_params = 0\n","    train_params = 0\n","    \n","    \n","    for name, parameter in model.named_parameters():\n","        params = parameter.numel()\n","        \n","        # trainable parameters\n","        if parameter.requires_grad:\n","            tr_p = params\n","            train_params += params \n","        else:\n","            tr_p = 0\n","        \n","        # total parameters\n","        total_params+=params\n","        \n","        output_config[\"parameters\"][name] = {}\n","        output_config[\"parameters\"][name][\"trainable\"] = params\n","        output_config[\"parameters\"][name][\"total\"] = tr_p\n","       \n","    output_config[\"total_trainable_parameters\"] =  train_params\n","    output_config[\"total_parameters\"] =  total_params\n","    \n","    \n","    return output_config"]},{"cell_type":"markdown","metadata":{},"source":["# Prediction"]},{"cell_type":"code","execution_count":140,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.863235Z","iopub.status.busy":"2023-01-18T05:30:31.862315Z","iopub.status.idle":"2023-01-18T05:30:31.882764Z","shell.execute_reply":"2023-01-18T05:30:31.881747Z","shell.execute_reply.started":"2023-01-18T05:30:31.863190Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def get_predictions(path_to_results, path_to_images, y, data_transforms, batch_size, device, fold=-2, color=\"gray\"):\n","    \"\"\"\n","    generates predictions\n","    \n","    path_to_results: where models are saved\n","    path_to_images: images dir\n","    y: (num_images, 1)\n","    data_transforms: transformation to apply\n","    fold: prediction for specific fold\n","    \n","    \n","    Returns:\n","        y_preds_folds: (num_folds, num_images)\n","        y_preds_probs_folds: (num_folds, num_images, num_classes) \n","        y_preds_en: (num_images,1)\n","        y_preds_probs_ensemble: (num_images, num_classes)\n","        \n","    thresholding could be added for binary (>th)/multi(>th for each class) classsifcation\n","    \"\"\"\n","    y_preds_probs_folds=[]\n","    y_preds_folds = []\n","    \n","    sigmoid = torch.nn.Sigmoid()\n","    softmax = torch.nn.Softmax(dim=1)       \n","    \n","    flag = False\n","    single_neuron = False\n","    \n","    for fold_no in sorted(os.listdir(path_to_results)):\n","        \n","        path_to_model = os.path.join(path_to_results, fold_no)\n","        \n","        if os.path.isdir(path_to_model):\n","            \n","            if fold == -2:\n","                flag = True\n","            else:\n","                if fold_no == str(fold):\n","                    flag = True\n","                else:\n","                    flag =False\n","            \n","            if flag:\n","                print(f\"Generating predicitons for fold no: {fold_no}\")\n","\n","                # get data\n","                data = ImageDataset(y, path_to_images, transform=data_transforms, color=color)\n","                dataloader = DataLoader(data, batch_size=batch_size, shuffle=False)\n","\n","                # load model\n","                model_path = os.path.join(path_to_model, \"model.pt\")\n","                print (f\"Loading model from {model_path}\")\n","                model = torch.load(model_path)\n","                model.to(device)\n","                model.eval()\n","\n","                with torch.no_grad():\n","\n","                    y_pred_prob=[]\n","                    y_pred_labels =[]\n","\n","                    for images, _ in dataloader:\n","                        images = images.to(device)\n","\n","                        y_pred = model(images)\n","\n","                        if y_pred.shape[-1] == 1:\n","                            # get probs\n","                            prob = sigmoid(y_pred)\n","\n","                            # get hard labels\n","                            labels = np.round(prob.detach().cpu().clone().numpy())\n","\n","                            single_neuron = True\n","                        else:\n","                            #get probs\n","                            prob = softmax(y_pred)\n","\n","                            # get hard labels\n","                            labels = np.argmax(prob.detach().cpu().clone().numpy(), axis=1) \n","\n","                        #combine iter results\n","                        y_pred_prob.extend(prob.detach().cpu().clone().numpy())\n","                        y_pred_labels.extend(labels)\n","\n","\n","                    # combine fold results\n","                    y_preds_folds.append(y_pred_labels)\n","                    y_preds_probs_folds.append(y_pred_prob)\n","\n","        \n","    # get ensemble results\n","    y_preds_probs_ensemble = np.mean(y_preds_probs_folds, axis=0)\n","    \n","\n","    if single_neuron:\n","        y_preds_en = np.round(y_preds_probs_ensemble).reshape(-1,1)\n","    else:\n","        y_preds_en = np.argmax(y_preds_probs_ensemble, axis=1).reshape(-1,1)\n","        \n","    return np.array(y_preds_folds), np.array(y_preds_probs_folds), np.array(y_preds_en), np.array(y_preds_probs_ensemble)\n"]},{"cell_type":"code","execution_count":141,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:31.977604Z","iopub.status.busy":"2023-01-18T05:30:31.976999Z","iopub.status.idle":"2023-01-18T05:30:31.991936Z","shell.execute_reply":"2023-01-18T05:30:31.990904Z","shell.execute_reply.started":"2023-01-18T05:30:31.977564Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def CNN_prediction(path_to_images, path_to_json, save_path):\n","    \"\"\"\n","    Generates prediction for the given data set\n","\n","    Parameters:\n","        path_to_images: path to the folder containing images on which prediction will be generated\n","        path_to_results: path to the folder where models are saved\n","        save_path: path to the folder where the results will be stored\n","        data_transform: transformation to apply\n","        batch_size: batch size\n","\n","    \"\"\"\n","\n","    if not os.path.isdir(path_to_images):\n","        raise ValueError(\"'path_to_images' must be a directory\")\n","\n","    if not os.path.exists(save_path):\n","        os.mkdir(save_path)\n","        print(f\"Created directory {save_path}\")\n","    else:\n","        print(f\"Warning: {save_path} already exists. Content may get overwritten\")\n","    \n","    # loading training pipeline\n","    pipeline = load_from_json(path_to_json)\n","\n","    pipeline[\"path_to_images\"] = path_to_images\n","    path_to_models = pipeline[\"path_to_results\"]\n","    \n","    pipeline[\"path_to_models\"] = path_to_models\n","    pipeline[\"path_to_results\"] = save_path\n","    \n","\n","    del pipeline['path_to_labels']\n","    \n","    device = pipeline[\"device\"]\n","    batch_size= pipeline[\"batch_size\"]\n","    classes = pipeline[\"classes\"]\n","    read_img_color = pipeline[\"read_img_color\"]\n","    \n","    \n","    # get transforms\n","    data_transform = pipeline[\"data_preprocessing\"][\"valid\"]\n","    data_transforms = data_preprocessing(data_transform)\n","    \n","    # generate y\n","    y=np.array(sorted(os.listdir(path_to_images))).reshape(-1,1)\n","    \n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    print(f\"Using Device: {device}\")\n","  \n","    print(\"\\nGenerating labels for the data . . . \")\n","    y_pred, y_pred_probs, y_pred_en, y_pred_probs_en = get_predictions(path_to_results=path_to_models, path_to_images=path_to_images, y=y, data_transforms=data_transforms, batch_size=batch_size, device=device, color=read_img_color)\n","    \n","   \n","    # transform labels and add image ids\n","    y_pred_tr = np.empty((y_pred.shape[0], y.shape[0], 2), dtype=y.dtype) \n","    for nf in range(y_pred.shape[0]):\n","        y_pred_tr[nf, :, 0] = y[:,0]\n","        y_pred_tr[nf, :, 1] = label_encoder(y=y_pred[nf,:], classes=classes, to_numbers=False)\n","    \n","    \n","    y_pred_probs_en = np.expand_dims(y_pred_probs_en, axis=0)\n","    y_pred_en = np.expand_dims (np.concatenate((y, y_pred_en), axis=1), axis=0)\n","    y_pred_en[0, :, 1] = label_encoder(y=y_pred_en[0, :, 1], classes=classes, to_numbers=False)\n","    \n","    print(f\"Generated labels for the data successfully. Shape: y_pred: {y_pred_tr.shape} y_pred_probs: {y_pred_probs.shape} y_pred_en: {y_pred_en.shape} y_pred_probs_en: {y_pred_probs_en.shape}\")\n","    \n","    \n","\n","    # save pipeline\n","    print(\"\\nsaving pipeline dictionary to json\")\n","    save_to_json(\n","        pipeline, \n","        save_path +\"/pipeline\"\n","        )\n","    \n","    # save labels\n","    print(\"\\nGenerating text file for the data\")\n","    generate_txt_file(\n","        y=y_pred_tr, \n","        path_to_results=save_path, \n","        name_of_file=\"labels\",\n","        y_probs = np.max(y_pred_probs, axis=-1)\n","        )\n","    \n","     # save labels with probs\n","    print(\"\\nGenerating text file for the data ensemble\")\n","    generate_txt_file(\n","        y=y_pred_en, \n","        path_to_results=save_path, \n","        name_of_file=\"labels_en\",\n","        y_probs = np.max(y_pred_probs_en, axis=-1)\n","        )\n","    \n","    \n","    pipeline[\"path_to_models\"] = path_to_results\n","    pipeline[\"path_to_results\"] = save_path\n","    \n","    print(\"\\nGenerated Predictions Successfully\")"]},{"cell_type":"markdown","metadata":{},"source":["# Training\n","\n","* for each epoch, train the model and evaluate the model on training and validaiton data using metrics: loss, balanced accuracy, mcc etc\n","\n","* store evaluation results for each epoch that will be used for learning curves\n","* save the best model using val loss or some other metric\n","\n","* generate prediction and prediction probabilities using best model on training and validation data\n","\n","save all the necessary info"]},{"cell_type":"code","execution_count":142,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:32.055895Z","iopub.status.busy":"2023-01-18T05:30:32.054935Z","iopub.status.idle":"2023-01-18T05:30:32.067684Z","shell.execute_reply":"2023-01-18T05:30:32.066541Z","shell.execute_reply.started":"2023-01-18T05:30:32.055831Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def train_epoch(model, device, dataloader, loss_fnt, optimizer):\n","    \"\"\"\n","    trains a model \n","    \n","    Parameters:\n","        model: model to be trained\n","        device: device on which model to be trained\n","        dataloader: of training dataset\n","        lost_fnt: loss function\n","        optimizer:\n","        \n","    Returns:\n","        train_loss, train_balanced_accu, train_mcc \n","        unnormalized values (No multiplication of 1/num_images)\n","    \n","    \"\"\"\n","    \n","    train_loss = 0\n","    train_balanced_accu = 0\n","    train_mcc = 0\n","    \n","    sigmoid = torch.nn.Sigmoid()\n","    \n","    model.train()\n","\n","    for images, labels in dataloader:\n","\n","        images,labels = images.to(device),labels.to(device)\n","        \n","        optimizer.zero_grad()\n","        \n","        output = model(images)\n","        \n","        loss = loss_fnt(output,labels)\n","        \n","        loss.backward()\n","        \n","        optimizer.step()\n","        \n","        train_loss += loss.item() * images.size(0)\n","        \n","        if output.shape[-1] == 1:\n","            y_pred = torch.round(sigmoid(output.detach().clone())).cpu().numpy()\n","        else:\n","            y_pred = np.argmax(output.detach().clone().cpu().numpy(), axis=1)\n","                    \n","        train_balanced_accu += accuracy(y_true=labels.detach().clone().cpu().numpy(), y_pred=y_pred, parameters={\"type\": \"balanced\"})[0] * images.size(0)\n","        train_mcc += mcc(y_true=labels.detach().clone().cpu().numpy(), y_pred=y_pred, parameters={})[0] * images.size(0) \n","\n","        \n","    return train_loss, train_balanced_accu, train_mcc"]},{"cell_type":"code","execution_count":143,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:32.104939Z","iopub.status.busy":"2023-01-18T05:30:32.104654Z","iopub.status.idle":"2023-01-18T05:30:32.114029Z","shell.execute_reply":"2023-01-18T05:30:32.113025Z","shell.execute_reply.started":"2023-01-18T05:30:32.104914Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def valid_epoch(model, device, dataloader, loss_fnt):\n","    \"\"\"\n","    Generates results on validation data\n","    \n","    Parameters:\n","        model: model to be used for evaluatioon\n","        device: device on which model will be evaluated\n","        dataloader: of validation dataset\n","        lost_fnt: loss function\n","        \n","    Returns:\n","        valid_loss, valid_balanced_accu, valid_mcc \n","        unnormalized values (No multiplication of 1/num_images)\n","    \n","    \"\"\"\n","    \n","    valid_loss = 0\n","    valid_balanced_accu = 0\n","    valid_mcc = 0\n","    \n","    sigmoid = torch.nn.Sigmoid()\n","    \n","    model.eval()\n","    \n","    with torch.no_grad():\n","        \n","        for images, labels in dataloader:\n","\n","            images,labels = images.to(device),labels.to(device)\n","\n","            output = model(images)\n","\n","            loss=loss_fnt(output,labels)\n","\n","            valid_loss+=loss.item()*images.size(0)\n","            \n","            if output.shape[-1] == 1:\n","                y_pred = torch.round(sigmoid(output.detach().clone())).cpu().numpy()\n","            else:\n","                y_pred = np.argmax(output.detach().clone().cpu().numpy(), axis=1)\n","            \n","            valid_balanced_accu += accuracy(y_true=labels.cpu().numpy(), y_pred=y_pred, parameters={\"type\": \"balanced\"})[0] * images.size(0)\n","            valid_mcc += mcc(y_true=labels.cpu().numpy(), y_pred=y_pred, parameters={})[0] * images.size(0) \n","\n","    return valid_loss, valid_balanced_accu, valid_mcc\n"]},{"cell_type":"code","execution_count":144,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:32.205602Z","iopub.status.busy":"2023-01-18T05:30:32.205337Z","iopub.status.idle":"2023-01-18T05:30:32.276305Z","shell.execute_reply":"2023-01-18T05:30:32.275385Z","shell.execute_reply.started":"2023-01-18T05:30:32.205571Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def CNN_training(pipeline):\n","    \n","    output_config = {}\n","    \n","    # set up whether to use cpu or gpu\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    print(f\"Using Device: {device}\")\n","    output_config[\"device\"] = device\n","    \n","    #get results folder\n","    path_to_results = pipeline[\"path_to_results\"]\n","    output_config[\"path_to_results\"] = path_to_results\n","    if not os.path.exists(path_to_results):\n","            os.mkdir(path_to_results)\n","    \n","    #get iamge folder\n","    path_to_images = pipeline[\"path_to_images\"]\n","    output_config[\"path_to_images\"] = path_to_images\n","    \n","    # get batch size\n","    batch_size= pipeline[\"batch_size\"]\n","    output_config[\"batch_size\"] =batch_size\n","    \n","    #reading image in gray scale or rgb\n","    read_img_color = pipeline[\"read_img_color\"]\n","    output_config[\"read_img_color\"] = read_img_color\n","    \n","    # get batch size\n","    num_epochs= pipeline[\"num_epochs\"]\n","    output_config[\"num_epochs\"] =num_epochs\n","    \n","    \n","    # read labels from txt\n","    path_to_labels = pipeline[\"path_to_labels\"]\n","    output_config[\"path_to_labels\"] = path_to_labels\n","\n","    y = np.loadtxt(path_to_labels, dtype=str, delimiter=\" \")\n","    \n","    # get unique labels and their ratios\n","    original_labels = {}\n","    unique_labels, counts = np.unique(y[:,1], return_counts=True)\n","    print(f\"Unique labels found and their frequencies: \")\n","    for ul, c in zip(unique_labels, counts):\n","        original_labels[ul] = format(c*100/y.shape[0], '.4f')\n","        print(f\"Label: {ul}, %age: {format(c*100/y.shape[0], '.4f')}\")\n","    output_config[\"original_labels\"] = original_labels\n","    \n","    # split data\n","    split_type = pipeline[\"split_type\"]\n","    output_config[\"split_type\"] = split_type\n","    \n","    if \"simple\" in split_type:\n","        test_size = pipeline[\"test_size\"]\n","        output_config[\"test_size\"] = test_size\n","        num_folds=1\n","        y_train, y_valid = split_data(y=y, split_type=split_type, test_size=test_size)\n","    \n","    elif \"fold\" in split_type:\n","        num_folds = pipeline[\"num_folds\"]\n","        output_config[\"num_folds\"] = num_folds\n","        y_train, y_valid = split_data(y=y, split_type=split_type, n_folds=num_folds)\n","    \n","    else:\n","        raise ValueError(\"Unknown spliting type\")\n","        \n","    print(f\"Split the data successfully. Shape: y_train: {y_train.shape} y_valid: {y_valid.shape}\")\n","        \n","        \n","    # number of images in train and valid data\n","    num_images_train = y_train.shape[1]\n","    num_images_valid = y_valid.shape[1]\n","    \n","    # transform labels\n","    classes = pipeline[\"classes\"]\n","    for nf in range(num_folds):\n","        y_train[nf, :, 1] = label_encoder(y=y_train[nf,:,1], classes=classes, to_numbers=True)\n","        y_valid[nf, :, 1] = label_encoder(y=y_valid[nf,:,1], classes=classes, to_numbers=True)\n","    \n","    output_config[\"classes\"] = classes.tolist()\n","    output_config[\"data_preprocessing\"]={}\n","    output_config[\"data_preprocessing\"][\"train\"] = OrderedDict()\n","    output_config[\"data_preprocessing\"][\"valid\"] = OrderedDict()\n","    \n","    # get preprocessing transformations for training data\n","    train_data_transform = pipeline[\"data_preprocessing\"][\"train\"]\n","    output_config[\"data_preprocessing\"][\"train\"] = train_data_transform\n","    \n","    train_data_transforms = data_preprocessing(train_data_transform)\n","    \n","        \n","    # get preprocessing transformations for validation data\n","    valid_data_transform = pipeline[\"data_preprocessing\"][\"valid\"]\n","    output_config[\"data_preprocessing\"][\"valid\"] = valid_data_transform\n","    \n","    valid_data_transforms = data_preprocessing(valid_data_transform)\n","    \n","     # get input shape for the model\n","    input_shape = pipeline[\"input_shape\"]\n","    output_config[\"input_shape\"] = input_shape\n","    \n","    # get layers of the model\n","    layers = pipeline[\"model\"]\n","    output_config[\"model\"] = layers\n","    \n","    use_pretrained_model = pipeline[\"use_pretrained_model\"]\n","    output_config[\"use_pretrained_model\"] = use_pretrained_model\n","    \n","    #get loss settings\n","    loss_param = pipeline[\"loss\"]\n","    output_config[\"loss\"] = loss_param\n","    \n","    # get optimizer settings\n","    optim_parameters = pipeline[\"optimizer\"]\n","    output_config[\"optimizer\"] =optim_parameters \n","    \n","    #get metrics\n","    metrics = pipeline[\"metrics\"]\n","    output_config[\"metrics\"] =metrics\n","    \n","    # get plots\n","    plots = pipeline[\"plots\"]\n","    output_config[\"plots\"] =plots\n","    \n","    \n","    # for saving scores of each fold\n","    train_loss_fold, train_accu_fold,train_mcc_fold = [],[],[]\n","    valid_loss_fold, valid_accu_fold,valid_mcc_fold = [],[],[]\n","    \n","    # for saving scores of best model of each model\n","    train_loss_fold_best, train_accu_fold_best,train_mcc_fold_best = [],[],[]\n","    valid_loss_fold_best, valid_accu_fold_best,valid_mcc_fold_best = [],[],[]\n","    best_model_epoch_no_fold = []\n","    \n","    train_labels ={}\n","    valid_labels={}\n","    class_weights_dict = {}\n","    \n","    for fold_no in range(num_folds):\n","        \n","        path_to_fold = os.path.join(path_to_results, str(fold_no))\n","        if not os.path.exists(path_to_fold):\n","            os.mkdir(path_to_fold)\n","        \n","        # get training data\n","        train_data = ImageDataset(y_train[fold_no], path_to_images, transform=train_data_transforms,color=read_img_color)\n","        train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","        \n","        # get validation data\n","        valid_data = ImageDataset(y_valid[fold_no], path_to_images, transform=valid_data_transforms, color=read_img_color)\n","        valid_dataloader = DataLoader(valid_data, batch_size=batch_size, shuffle=False)\n","        \n","        # load model\n","        model = Model(input_shape,layers, use_pretrained_model)\n","        model.to(device)\n","        model_parameters = count_parameters(model)\n","        \n","        if fold_no == 0:\n","            output_config[\"model_parameters\"] = model_parameters\n","        \n","        trainable_params = model_parameters[\"total_trainable_parameters\"]\n","        total_params = model_parameters[\"total_parameters\"]\n","    \n","\n","        print(f\"\\nFold No: {fold_no+1} Model: \\n {model}\")\n","        print(f\"Total trainable parameters: {trainable_params} Total parameters: {total_params}\")\n","        \n","        # get ratio of each class in y_train\n","        train_labels[str(fold_no)] = {}\n","        unique_labels, counts = np.unique(y_train[fold_no,:,1], return_counts=True)\n","        print(f\"\\nFold No: {fold_no+1} Training data: Unique labels: \")\n","        for ul, c in zip(unique_labels, counts):\n","                train_labels[str(fold_no)][ul] = format(c*100/num_images_train, '.4f')\n","                print(f\"Label: {ul}, %age: {format(c*100/num_images_train, '.4f')}\")\n","\n","        # get ratio of each class in y_valid\n","        valid_labels[str(fold_no)] = {}\n","        unique_labels, counts =np.unique(y_valid[fold_no, :,1], return_counts=True)\n","        print(f\"\\nFold No: {fold_no+1} Validation data: Unique labels: \")\n","        for ul, c in zip(unique_labels, counts):\n","            valid_labels[str(fold_no)][ul] = format(c*100/num_images_valid, '.4f')\n","            print(f\"Label: {ul}, %age: {format(c*100/num_images_valid, '.4f')}\")\n","        \n","        # setup loss\n","        if loss_param[\"use_single_neuron\"]:\n","            class_weights = [float(train_labels[str(fold_no)]['0']) / float(train_labels[str(fold_no)]['1'])]\n","            \n","            loss_fnt = setup_loss({**loss_param, **{\"class_weights\": class_weights, \"device\": device}})\n","            print(f\"\\nFold No: {fold_no+1} loss: {loss_fnt} weights: {loss_fnt.pos_weight}\")\n","        \n","        else:\n","            class_weights = []\n","            for cl in range(len(classes)):\n","                class_weights.append(1 - float(train_labels[str(fold_no)][str(cl)])/100 )\n","\n","            loss_fnt = setup_loss({**loss_param, **{\"class_weights\": class_weights, \"device\": device}})\n","            print(f\"\\nFold No: {fold_no+1} loss: {loss_fnt} weights: {loss_fnt.weight}\")\n","        \n","        class_weights[fold_no] = class_weights\n","\n","        # setup optimizer\n","        optimizer = setup_optimizer(model.parameters(), optim_parameters)\n","        print(f\"\\nFold No: {fold_no+1} Optimizer: \\n {optimizer}\")\n","          \n","        # for saving scores of each epoch of best model\n","        train_loss_epoch, train_accu_epoch,train_mcc_epoch = [],[],[]\n","        valid_loss_epoch, valid_accu_epoch,valid_mcc_epoch = [],[],[]\n","        \n","        for epoch in range(num_epochs):\n","            \n","            print(f\"Training :Fold No: {fold_no+1}/{num_folds} Epoch:{epoch+1}/{num_epochs} . . .\")\n","            \n","            train_loss, train_accu, train_mcc = train_epoch(model, device, train_dataloader, loss_fnt, optimizer)\n","            valid_loss, valid_accu, valid_mcc = valid_epoch(model, device, valid_dataloader, loss_fnt)\n","            \n","            train_loss, train_accu, train_mcc = train_loss/num_images_train, train_accu/num_images_train, train_mcc/num_images_train\n","            valid_loss, valid_accu, valid_mcc = valid_loss/num_images_valid, valid_accu/num_images_valid, valid_mcc/num_images_valid\n","            \n","            print(f'Evaluated:Fold No: {fold_no+1}/{num_folds} Epoch:{epoch+1}/{num_epochs} Training: Loss: {np.around(train_loss, 4)}, Bal_accu: {np.around(train_accu, 4)}, mcc: {np.around(train_mcc, 4)} Validation: Loss: {np.around(valid_loss, 4)}, Bal_accu: {np.around(valid_accu, 4)}, mcc: {np.around(valid_mcc, 4)}')\n","             \n","            # save best model\n","            if epoch == 0:\n","                print(f\"Saving Model of first epoch\")\n","                \n","                torch.save(model, os.path.join(path_to_fold, \"model.pt\"))\n","                \n","                # save best model scores\n","                best_model_epoch_no = epoch\n","                train_loss_epoch_best = train_loss\n","                train_accu_epoch_best = train_accu\n","                train_mcc_epoch_best = train_mcc\n","\n","                valid_loss_epoch_best = valid_loss\n","                valid_accu_epoch_best = valid_accu\n","                valid_mcc_epoch_best = valid_mcc   \n","\n","            elif  valid_loss_epoch_best > valid_loss:\n","                \n","                print(f\"Validation Loss score improved. Saving Model.\")\n","                \n","                torch.save(model, os.path.join(path_to_fold, \"model.pt\"))\n","                \n","                # save best model scores\n","                best_model_epoch_no = epoch\n","                train_loss_epoch_best = train_loss\n","                train_accu_epoch_best = train_accu\n","                train_mcc_epoch_best = train_mcc\n","\n","                valid_loss_epoch_best = valid_loss\n","                valid_accu_epoch_best = valid_accu\n","                valid_mcc_epoch_best = valid_mcc   \n","\n","            else:\n","                print(\"Validaiton Loss score did not improve.\")\n","      \n","            # save scores for learning curves\n","            train_loss_epoch.append(train_loss)\n","            train_accu_epoch.append(train_accu)\n","            train_mcc_epoch.append(train_mcc)\n","            \n","            valid_loss_epoch.append(valid_loss)\n","            valid_accu_epoch.append(valid_accu)\n","            valid_mcc_epoch.append(valid_mcc)\n","            \n","        print(f\"\\nFold No: {fold_no+1}/{num_folds} Best Validation Loss and balanced accuracy recorded on epoch {best_model_epoch_no+1}: {valid_loss_epoch_best} {valid_accu_epoch_best}\")\n","\n","        # save best model for each fold\n","        best_model_epoch_no_fold.append(best_model_epoch_no)\n","        \n","        train_loss_fold_best.append(train_loss_epoch_best)\n","        train_accu_fold_best.append(train_accu_epoch_best)\n","        train_mcc_fold_best.append(train_mcc_epoch_best)\n","\n","        valid_loss_fold_best.append(valid_loss_epoch_best)\n","        valid_accu_fold_best.append(valid_accu_epoch_best)\n","        valid_mcc_fold_best.append(valid_mcc_epoch_best)\n","        \n","        # save scores for each fold\n","        train_loss_fold.append(train_loss_epoch)\n","        train_accu_fold.append(train_accu_epoch)\n","        train_mcc_fold.append(train_mcc_epoch)\n","\n","        valid_loss_fold.append(valid_loss_epoch)\n","        valid_accu_fold.append(valid_accu_epoch)\n","        valid_mcc_fold.append(valid_mcc_epoch)\n","    \n","    \n","    #convert to numpy\n","    train_loss_fold = np.array(train_loss_fold)\n","    train_accu_fold = np.array(train_accu_fold)\n","    train_mcc_fold = np.array(train_mcc_fold)\n","    \n","    valid_loss_fold = np.array(valid_loss_fold)\n","    valid_accu_fold = np.array(valid_accu_fold)\n","    valid_mcc_fold = np.array(valid_mcc_fold)\n","    \n","    \n","    \n","    # generate learning curves array\n","    score_for_learning_curves = np.empty((num_folds, 3,2, num_epochs), dtype=train_loss_fold.dtype)\n","    for nf in range(num_folds):\n","        score_for_learning_curves[nf, 0, 0, :] = train_loss_fold[nf]\n","        score_for_learning_curves[nf, 1, 0, :] = train_accu_fold[nf]\n","        score_for_learning_curves[nf, 2, 0, :] = train_mcc_fold[nf]\n","        \n","        score_for_learning_curves[nf, 0, 1, :] = valid_loss_fold[nf]\n","        score_for_learning_curves[nf, 1, 1, :] = valid_accu_fold[nf]\n","        score_for_learning_curves[nf, 2, 1, :] = valid_mcc_fold[nf]\n","    \n","    print(f\"\\nTrained the model for all folds. Shape: train: loss: {train_loss_fold.shape} accu: {train_accu_fold.shape} mcc: {train_mcc_fold.shape} valid: loss: {valid_loss_fold.shape} accu: {valid_accu_fold.shape} mcc: {valid_mcc_fold.shape} Learning scores: {score_for_learning_curves.shape}\")\n","    \n","    output_config[\"train_labels\"] = train_labels\n","    output_config[\"valid_labels\"] = valid_labels\n","    \n","    output_config[\"results\"] = {}\n","    output_config[\"results\"][\"best_model\"]={}\n","    output_config[\"results\"][\"best_model\"][\"epoch_number\"]=best_model_epoch_no_fold\n","    output_config[\"results\"][\"best_model\"][\"train\"]={}\n","    output_config[\"results\"][\"best_model\"][\"train\"][\"loss\"]=train_loss_fold_best\n","    output_config[\"results\"][\"best_model\"][\"train\"][\"balanced_accuracy\"]=train_accu_fold_best\n","    output_config[\"results\"][\"best_model\"][\"train\"][\"mcc\"]=train_mcc_fold_best\n","    output_config[\"results\"][\"best_model\"][\"valid\"]={}\n","    output_config[\"results\"][\"best_model\"][\"valid\"][\"loss\"]=valid_loss_fold_best\n","    output_config[\"results\"][\"best_model\"][\"valid\"][\"balanced_accuracy\"]=valid_accu_fold_best\n","    output_config[\"results\"][\"best_model\"][\"valid\"][\"mcc\"]=valid_mcc_fold_best\n","    \n","    # generate predicitons\n","    train_pred = []\n","    train_pred_prob = []\n","    valid_pred = []\n","    valid_pred_prob = []\n","    for fn in range(num_folds):\n","        print(\"\\nGenerating Predictions on training data\")\n","        train_pred_fn, train_pred_prob_fn, _,_= get_predictions(path_to_results=path_to_results, path_to_images=path_to_images, y=y_train[fn], data_transforms=valid_data_transforms, fold=fn, batch_size=batch_size, device=device, color=read_img_color)\n","        print(\"Generating Predictions on validation data\")\n","        valid_pred_fn, valid_pred_prob_fn, _,_= get_predictions(path_to_results=path_to_results, path_to_images=path_to_images, y=y_valid[fn], data_transforms=valid_data_transforms, fold=fn, batch_size=batch_size, device=device, color=read_img_color)\n","        \n","        train_pred_fn = label_encoder(y=train_pred_fn, classes=classes, to_numbers=False)\n","        valid_pred_fn = label_encoder(y=valid_pred_fn, classes=classes, to_numbers=False)\n","     \n","        \n","        train_pred.append([y_train[fn, :, 0], train_pred_fn])\n","        train_pred_prob.append(train_pred_prob_fn[0])\n","        valid_pred.append([y_valid[fn, :, 0],valid_pred_fn])\n","        valid_pred_prob.append(valid_pred_prob_fn[0])\n","        \n","\n","    train_pred =np.array(train_pred).swapaxes(-1,-2)\n","    train_pred_prob =np.array(train_pred_prob)\n","    valid_pred =np.array(valid_pred).swapaxes(-1,-2)\n","    valid_pred_prob =np.array(valid_pred_prob)\n","    \n","    print(f\"\\nGenerated predictions on training and validation dataset. Shape: train: labels: {train_pred.shape} prob: {train_pred_prob.shape} valid: labels: {valid_pred.shape} prob: {valid_pred_prob.shape}\")    \n","    \n","    # transform int labels to name of classes\n","    for nf in range(num_folds):\n","        y_train[nf, :, 1] = label_encoder(y=y_train[nf,:,1], classes=classes, to_numbers=False)\n","        y_valid[nf, :, 1] = label_encoder(y=y_valid[nf,:,1], classes=classes, to_numbers=False)\n","        \n","    \n","     # generate predicitons on complete dataset\n","    print(\"\\nGenerating predicitons on complete dataset. . . \")\n","    \n","    # transform name of classes to int\n","    y[:, 1] = label_encoder(y=y[:,1], classes=classes, to_numbers=True)\n","    \n","    y_pred_comp, y_pred_prob_comp, y_pred_en, y_pred_prob_en = get_predictions(path_to_results=path_to_results, path_to_images=path_to_images, y=y, data_transforms=valid_data_transforms, device=device, batch_size=batch_size, color=read_img_color)\n","    \n","    \n","    # transform labels and add image ids\n","    y_pred_com = np.empty((num_folds, y.shape[0], 2), dtype=y.dtype) \n","    for nf in range(num_folds):\n","        y_pred_com[nf, :, 0] = y[:,0]\n","        y_pred_com[nf, :, 1] = label_encoder(y=y_pred_comp[nf], classes=classes, to_numbers=False)\n","    \n","    y_pred_prob_com = y_pred_prob_comp\n","        \n","    y_pred_en = np.expand_dims (np.concatenate((y[:,0].reshape(-1,1), y_pred_en), axis=1), axis=0)\n","    y_pred_en[0, :, 1] = label_encoder(y=y_pred_en[0, :, 1], classes=classes, to_numbers=False)\n","    \n","    y_pred_prob_en = np.expand_dims(y_pred_prob_en, axis=0)   \n","    \n","    y = np.repeat(y[None, :,:], num_folds, axis=0)\n","    \n","    print(f\"\\nGenerated predicitons on complete dataset. Shape: Folds: labels: {y_pred_com.shape} probs: {y_pred_prob_com.shape} Ensembles: labels: {y_pred_en.shape} probs: {y_pred_prob_en.shape}\")\n","    \n","    \n","    #evaluate metrics on training data\n","    print(\"\\nEvaluating Metrics on training data . . . \")\n","    metrics_train, metrics_train_config, metrics_train_list = evaluate_metrics(\n","        y_true=y_train, \n","        y_pred=train_pred, \n","        metrics=metrics,\n","        y_pred_probs = train_pred_prob\n","        )\n","    print(\"\\nResults\")\n","\n","\n","    for fold_no in range(num_folds):\n","        for met_no, met in enumerate(metrics_train_list):\n","            print(f\"Fold No: {fold_no} Metric: {met}  Score: {np.around(metrics_train[fold_no, met_no], 4)} \")\n","\n","    print(f\"Evaluated Metrics on the training data successfully. Shape:{metrics_train.shape}\")\n","\n","    #evaluate metrics on validataion data\n","    print(\"\\nEvaluating Metrics on validation data . . . \")\n","    metrics_valid, _, metrics_valid_list = evaluate_metrics(\n","        y_true=y_valid, \n","        y_pred=valid_pred, \n","        metrics=metrics,\n","        y_pred_probs = valid_pred_prob\n","        )\n","    print(\"\\nResults\")\n","    \n","    for fold_no in range(num_folds):\n","        for met_no, met in enumerate(metrics_valid_list):\n","            print(f\"Fold No: {fold_no} Metric: {met}  Score: {np.around(metrics_valid[fold_no, met_no],4)} \")\n","\n","    print(f\"Evaluated Metrics on the validation data successfully. Shape: {metrics_valid.shape}\")\n","    \n","    print(\"\\nEvaluating Metrics on complete data . . . \")\n","    # transform int labels to name of classes\n","    for nf in range(num_folds):\n","        y[nf,:, 1] = label_encoder(y=y[nf,:,1], classes=classes, to_numbers=False)\n","    \n","    metrics_com, metrics_com_config, metrics_com_list = evaluate_metrics(\n","        y_true=y, \n","        y_pred=y_pred_com, \n","        metrics=metrics,\n","        y_pred_probs = y_pred_prob_com\n","        )\n","    print(\"\\nResults\")\n","    \n","    for nf in range(num_folds):\n","        for met_no, met in enumerate(metrics_com_list):\n","            print(f\"Fold No: {nf} Metric: {met}  Score: {np.around(metrics_com[nf, met_no],4)} \")\n","            \n","    print(f\"Evaluated Metrics on the complete dataset successfully. Shape:{metrics_com.shape}\")\n","    \n","    print(\"\\nEvaluating Metrics on complete dataset using ensembles . . . \") \n","    metrics_en, metrics_en_config, metrics_en_list = evaluate_metrics(\n","        y_true=np.expand_dims(y[0], axis=0), \n","        y_pred=y_pred_en, \n","        metrics=metrics,\n","        y_pred_probs = y_pred_prob_en\n","        )\n","    print(\"\\nResults\")\n","    \n","    for met_no, met in enumerate(metrics_en_list):\n","            print(f\"Fold No: {0} Metric: {met}  Score: {np.around( metrics_en[0, met_no], 4)} \")\n","            \n","    print(f\"Evaluated Metrics on the complete dataset using ensemble successfully. Shape:{metrics_en.shape}\")\n","\n","            \n","    # create plots\n","    print(\"\\nCreating Plots for training data . . .\")\n","    plots_train_config = create_plots(\n","        y_true=y_train, \n","        y_pred=train_pred, \n","        plots= plots, \n","        path_to_results=path_to_results,\n","        path_to_images=path_to_images,\n","        name_of_file = \"train\",\n","        training_metric_scores=score_for_learning_curves,\n","        y_pred_probs = train_pred_prob\n","        )\n","    print(\"Created Plots for training data\")\n","\n","    print(\"\\nCreating Plots for validation data\")\n","    _ = create_plots(\n","        y_true=y_valid, \n","        y_pred=valid_pred, \n","        plots= plots, \n","        path_to_results=path_to_results,\n","        path_to_images=path_to_images, \n","        name_of_file = \"valid\",\n","        y_pred_probs = valid_pred_prob\n","        )\n","    print(\"Created Plots for validation data\")\n","    \n","    print(\"\\nCreating Plots for complete dataset\")\n","    _ = create_plots(\n","        y_true=y, \n","        y_pred=y_pred_com, \n","        plots= plots, \n","        path_to_results=path_to_results,\n","        path_to_images=path_to_images, \n","        name_of_file = \"complete\",\n","        y_pred_probs = y_pred_prob_com\n","        )\n","    print(\"Created Plots for complete dataset\")\n","    \n","    print(\"\\nCreating Plots for ensemble dataset\")\n","    _ = create_plots(\n","        y_true=np.expand_dims(y[0], axis=0), \n","        y_pred=y_pred_en, \n","        plots= plots, \n","        path_to_results=path_to_results,\n","        path_to_images=path_to_images, \n","        name_of_file = \"ensemble\",\n","        y_pred_probs = y_pred_prob_en\n","        )\n","    print(\"Created Plots for ensemble data\")\n","\n","    # save pipeline\n","    print(\"\\nsaving pipeline dictionary to json\")\n","    save_to_json(\n","        output_config, \n","        os.path.join(path_to_results, \"training_pipeline\")\n","        )\n","    \n","        # save labels\n","    print(\"\\nGenerating text file for training data\")\n","    generate_txt_file(\n","        y=train_pred, \n","        path_to_results=path_to_results, \n","        name_of_file=\"train\",\n","        y_probs = np.max(train_pred_prob, axis=-1)\n","        )\n","\n","    print(\"\\nGenerating text file for validation data\")\n","    generate_txt_file(\n","        y=valid_pred, \n","        path_to_results=path_to_results,  \n","        name_of_file=\"valid\",\n","        y_probs = np.max(valid_pred_prob, axis=-1)\n","        )\n","    \n","    print(\"\\nGenerating text file for complete data\")\n","    generate_txt_file(\n","        y=y_pred_com, \n","        path_to_results=path_to_results,  \n","        name_of_file=\"complete\",\n","        y_probs = np.max(y_pred_prob_com, axis=-1)\n","        )\n","    \n","    print(\"\\nGenerating text file for ensemble data\")\n","    generate_txt_file(\n","        y=y_pred_en, \n","        path_to_results=path_to_results,  \n","        name_of_file=\"ensemble\",\n","        y_probs = np.max(y_pred_prob_en, axis=-1)\n","        )\n","    \n","    # save metrics train and metrics_valid\n","    print(\"\\nSaving training results \")\n","    save_results(\n","        results=metrics_train,\n","        metrics=metrics_train_list,\n","        path_to_results=path_to_results,\n","        name_of_file=\"train\"\n","    )\n","\n","    print(\"\\nSaving validation results \")\n","    save_results(\n","        results=metrics_valid,\n","        metrics=metrics_valid_list,\n","        path_to_results=path_to_results,\n","        name_of_file=\"valid\"\n","    )\n","    \n","    print(\"\\nSaving complete data results \")\n","    save_results(\n","        results=metrics_com,\n","        metrics=metrics_com_list,\n","        path_to_results=path_to_results,\n","        name_of_file=\"complete\"\n","    )\n","    \n","    print(\"\\nSaving ensemble results \")\n","    save_results(\n","        results=metrics_en,\n","        metrics=metrics_en_list,\n","        path_to_results=path_to_results,\n","        name_of_file=\"ensemble\"\n","    )\n","\n","    print(\"\\nTraining Completed\\n\")\n","    \n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Save\n","* the model and all the info (model, parameters, results etc)"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true}},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Evaluate Metrics\n","Using the best model, evaluate the metircs and save the find results in csv file"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Plots:\n","* learning curve\n","* metrics\n","* confusion matrix\n","* misidentified samples\n","* ROC\n","* maybe weights\n","\n","save them"]},{"cell_type":"markdown","metadata":{},"source":["# Prediction\n","load the best model, generate prediction on train, test and noisy test dataset and create two txt files:\n","- one with file name and label (like the one provided)\n","- one with file name, label and probability"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Additional notes:\n","* set up excel sheet for finding optimal hyperparameers in organized manner\n","* set up excel for tracking results of different models in organized manner\n","* or just find some function that finds optimal hyperparameters\n","\n","* Results may not stay saved if kaggle session is closed. Either download them in timely manner or find a way to save them"]},{"cell_type":"markdown","metadata":{},"source":["# Results Directory"]},{"cell_type":"code","execution_count":145,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:32.700814Z","iopub.status.busy":"2023-01-18T05:30:32.700034Z","iopub.status.idle":"2023-01-18T05:30:32.708880Z","shell.execute_reply":"2023-01-18T05:30:32.707966Z","shell.execute_reply.started":"2023-01-18T05:30:32.700772Z"},"trusted":true},"outputs":[],"source":["run_name = 'ResNet50'\n","\n","path_to_result ='/home/ahmad/Documents/TUHH/Semester 3/Intelligent Systems in Medicine/Project/Classification-of-different-dieases-using-ML-and-DL/results/Phase2_results/local_comp'\n","if not os.path.exists(path_to_result):\n","    os.mkdir(path_to_result)\n","    \n","path_to_results = os.path.join(path_to_result, run_name)\n","if not os.path.exists(path_to_results):\n","    os.mkdir(path_to_results)\n","    \n","path_to_binary_results = os.path.join(path_to_results, \"binary\")\n","if not os.path.exists(path_to_binary_results):\n","    os.mkdir(path_to_binary_results)\n","\n","path_to_multiclass_results = os.path.join(path_to_results, \"multi\")\n","if not os.path.exists(path_to_multiclass_results):\n","    os.mkdir(path_to_multiclass_results)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Raw Data Directory"]},{"cell_type":"code","execution_count":146,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:32.825568Z","iopub.status.busy":"2023-01-18T05:30:32.825271Z","iopub.status.idle":"2023-01-18T05:30:32.831721Z","shell.execute_reply":"2023-01-18T05:30:32.830678Z","shell.execute_reply.started":"2023-01-18T05:30:32.825541Z"},"trusted":true},"outputs":[],"source":["# path to data folders\n","path_to_data = \"/home/ahmad/Documents/TUHH/Semester 3/Intelligent Systems in Medicine/Project/Classification-of-different-dieases-using-ML-and-DL/data/raw_data\"\n","\n","# training data\n","path_to_train_data = os.path.join(path_to_data, \"train\")\n","path_to_binary_labels = os.path.join(path_to_data, \"train_binary.txt\")\n","path_to_multi_labels = os.path.join(path_to_data, \"train_multi.txt\")\n","\n","# testing data\n","path_to_test_data = os.path.join(path_to_data, \"test\")\n","\n","# noisy test data\n","path_to_noisy_test_data = os.path.join(path_to_data, \"noisy_test\")"]},{"cell_type":"code","execution_count":147,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:32.878453Z","iopub.status.busy":"2023-01-18T05:30:32.876593Z","iopub.status.idle":"2023-01-18T05:30:33.346694Z","shell.execute_reply":"2023-01-18T05:30:33.345637Z","shell.execute_reply.started":"2023-01-18T05:30:32.878426Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of images in: training data: 16930 test data: 4235 noisy test data: 4235\n"]}],"source":["num_train_data = len(os.listdir(path_to_train_data))\n","num_test_data = len(os.listdir(path_to_test_data))\n","num_noisy_test_data = len(os.listdir(path_to_noisy_test_data))\n","print(f\"Number of images in: training data: {num_train_data} test data: {num_test_data} noisy test data: {num_test_data}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Binary Classification"]},{"cell_type":"markdown","metadata":{},"source":["## Train "]},{"cell_type":"code","execution_count":148,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:33.349231Z","iopub.status.busy":"2023-01-18T05:30:33.348864Z","iopub.status.idle":"2023-01-18T05:30:33.375203Z","shell.execute_reply":"2023-01-18T05:30:33.374149Z","shell.execute_reply.started":"2023-01-18T05:30:33.349195Z"},"trusted":true},"outputs":[],"source":["#--------------------------------------Pipeline-------------------------------\n","\n","pipeline_binary = {}\n","pipeline_binary[\"path_to_results\"] = os.path.join(path_to_binary_results, \"train\")\n","\n","pipeline_binary[\"batch_size\"] = 32\n","pipeline_binary[\"num_epochs\"] = 20\n","\n","# for model\n","pipeline_binary[\"input_shape\"] = (224, 224,3)#for mobile net \n","\n","# path to folder containing images \n","pipeline_binary[\"path_to_images\"] = path_to_train_data\n","# path to labels.txt\n","pipeline_binary[\"path_to_labels\"] = path_to_binary_labels\n","\n","# split data\n","pipeline_binary[\"split_type\"] = \"simpleStratified\" #\"simple\", \"simpleStratified\", \"kfold\", \"kfoldStratified\"\n","pipeline_binary[\"test_size\"] = 0.3\n","\n","# names of the class\n","pipeline_binary[\"classes\"] = np.array([\"NO_COVID\", \"COVID\"])\n","pipeline_binary[\"use_pretrained_model\"]= True\n","pipeline_binary[\"read_img_color\"] = \"rgb\"\n","\n","# ---------------------------------set up data preprocessing methods and parameters------------------------------------\n","pipeline_binary[\"data_preprocessing\"] = {}\n","pipeline_binary[\"data_preprocessing\"][\"train\"] = OrderedDict()\n","pipeline_binary[\"data_preprocessing\"][\"train\"][\"normalize_0_1\"] = {}\n","pipeline_binary[\"data_preprocessing\"][\"train\"][\"normalize_0_1\"][\"name\"] = \"normalize\"\n","pipeline_binary[\"data_preprocessing\"][\"train\"][\"normalize_0_1\"][\"mean\"] = 0\n","pipeline_binary[\"data_preprocessing\"][\"train\"][\"normalize_0_1\"][\"std\"] = 255\n","\n","pipeline_binary[\"data_preprocessing\"][\"train\"][\"random_rotation\"] = {}\n","pipeline_binary[\"data_preprocessing\"][\"train\"][\"random_rotation\"][\"name\"] =  \"random_rotation\"\n","pipeline_binary[\"data_preprocessing\"][\"train\"][\"random_rotation\"][\"degrees\"] = 90\n","pipeline_binary[\"data_preprocessing\"][\"train\"][\"random_rotation\"][\"expand\"] = True\n","\n","pipeline_binary[\"data_preprocessing\"][\"train\"][\"random_crop\"] = {}\n","pipeline_binary[\"data_preprocessing\"][\"train\"][\"random_crop\"][\"name\"] = \"random_resized_crop\"\n","pipeline_binary[\"data_preprocessing\"][\"train\"][\"random_crop\"][\"output_shape\"] = 224\n","\n","pipeline_binary[\"data_preprocessing\"][\"train\"][\"horizontal_flip\"] = {}\n","pipeline_binary[\"data_preprocessing\"][\"train\"][\"horizontal_flip\"][\"name\"] = \"random_horizontal_flip\"\n","\n","pipeline_binary[\"data_preprocessing\"][\"train\"][\"vertical_flip\"] = {}\n","pipeline_binary[\"data_preprocessing\"][\"train\"][\"vertical_flip\"][\"name\"] = \"random_vertical_flip\"\n","\n","pipeline_binary[\"data_preprocessing\"][\"train\"][\"normalize_model_values\"] = {}\n","pipeline_binary[\"data_preprocessing\"][\"train\"][\"normalize_model_values\"][\"name\"] = \"normalize\"\n","pipeline_binary[\"data_preprocessing\"][\"train\"][\"normalize_model_values\"][\"mean\"] = [0.485, 0.456, 0.406]\n","pipeline_binary[\"data_preprocessing\"][\"train\"][\"normalize_model_values\"][\"std\"] = [0.229, 0.224, 0.225]\n","\n","\n","pipeline_binary[\"data_preprocessing\"][\"valid\"] = OrderedDict()\n","\n","pipeline_binary[\"data_preprocessing\"][\"valid\"][\"normalize_0_1\"] = {}\n","pipeline_binary[\"data_preprocessing\"][\"valid\"][\"normalize_0_1\"][\"name\"] = \"normalize\"\n","pipeline_binary[\"data_preprocessing\"][\"valid\"][\"normalize_0_1\"][\"mean\"] = 0\n","pipeline_binary[\"data_preprocessing\"][\"valid\"][\"normalize_0_1\"][\"std\"] = 255\n","\n","pipeline_binary[\"data_preprocessing\"][\"valid\"][\"center_crop\"] = {}\n","pipeline_binary[\"data_preprocessing\"][\"valid\"][\"center_crop\"][\"name\"] = \"center_crop\"\n","pipeline_binary[\"data_preprocessing\"][\"valid\"][\"center_crop\"][\"output_shape\"] = (224,224)\n","\n","pipeline_binary[\"data_preprocessing\"][\"valid\"][\"normalize_model_values\"] = {}\n","pipeline_binary[\"data_preprocessing\"][\"valid\"][\"normalize_model_values\"][\"name\"] = \"normalize\"\n","pipeline_binary[\"data_preprocessing\"][\"valid\"][\"normalize_model_values\"][\"mean\"] = [0.485, 0.456, 0.406]\n","pipeline_binary[\"data_preprocessing\"][\"valid\"][\"normalize_model_values\"][\"std\"] = [0.229, 0.224, 0.225]\n","\n","\n","\n","# ---------------------------------set up networks and parameters------------------------------------\n","\n","pipeline_binary[\"model\"] = OrderedDict()\n","pipeline_binary[\"model\"][\"resnet50\"] = {}\n","pipeline_binary[\"model\"][\"resnet50\"][\"name\"] = \"resnet50\"\n","pipeline_binary[\"model\"][\"resnet50\"][\"num_output_neurons\"] = 2\n","#pipeline_binary[\"model\"][\"mobile_net\"][\"num_layers_to_train\"] = 5\n","\n","\n","#pipeline_binary[\"model\"][\"conv1\"] = {}\n","#pipeline_binary[\"model\"][\"conv1\"][\"name\"] = \"conv\"\n","#pipeline_binary[\"model\"][\"conv1\"][\"number_of_kernels\"] = 32\n","#pipeline_binary[\"model\"][\"conv1\"][\"kernel_size\"] = 7\n","#pipeline_binary[\"model\"][\"lrelu1\"] = {}\n","#pipeline_binary[\"model\"][\"lrelu1\"][\"name\"] = \"lrelu\"\n","#pipeline_binary[\"model\"][\"maxpool1\"] = {}\n","#pipeline_binary[\"model\"][\"maxpool1\"][\"name\"] = \"max_pool\"\n","#pipeline_binary[\"model\"][\"maxpool1\"][\"kernel_size\"] = 2\n","\n","\n","#pipeline_binary[\"model\"][\"conv2\"] = {}\n","#pipeline_binary[\"model\"][\"conv2\"][\"name\"] = \"conv\"\n","#pipeline_binary[\"model\"][\"conv2\"][\"number_of_kernels\"] = 64\n","#pipeline_binary[\"model\"][\"conv2\"][\"kernel_size\"] = 5\n","#pipeline_binary[\"model\"][\"lrelu2\"] = {}\n","#pipeline_binary[\"model\"][\"lrelu2\"][\"name\"] = \"lrelu\"\n","#pipeline_binary[\"model\"][\"maxpool2\"] = {}\n","#pipeline_binary[\"model\"][\"maxpool2\"][\"name\"] = \"max_pool\"\n","#pipeline_binary[\"model\"][\"maxpool2\"][\"kernel_size\"] = 2\n","\n","\n","#pipeline_binary[\"model\"][\"conv3\"] = {}\n","#pipeline_binary[\"model\"][\"conv3\"][\"name\"] = \"conv\"\n","#pipeline_binary[\"model\"][\"conv3\"][\"number_of_kernels\"] = 128\n","#pipeline_binary[\"model\"][\"conv3\"][\"kernel_size\"] = 3\n","#pipeline_binary[\"model\"][\"lrelu3\"] = {}\n","#pipeline_binary[\"model\"][\"lrelu3\"][\"name\"] = \"lrelu\"\n","#pipeline_binary[\"model\"][\"maxpool3\"] = {}\n","#pipeline_binary[\"model\"][\"maxpool3\"][\"name\"] = \"max_pool\"\n","#pipeline_binary[\"model\"][\"maxpool3\"][\"kernel_size\"] = 3\n","\n","#pipeline_binary[\"model\"][\"flatten4\"] = {}\n","#pipeline_binary[\"model\"][\"flatten4\"][\"name\"] = \"flatten\"\n","\n","#pipeline_binary[\"model\"][\"linear5\"] = {}\n","#pipeline_binary[\"model\"][\"linear5\"][\"name\"] = \"linear\"\n","#pipeline_binary[\"model\"][\"linear5\"][\"neurons\"] = 256\n","#pipeline_binary[\"model\"][\"lrelu5\"] = {}\n","#pipeline_binary[\"model\"][\"lrelu5\"][\"name\"] = \"lrelu\"\n","\n","#pipeline_binary[\"model\"][\"linear6\"] = {}\n","#pipeline_binary[\"model\"][\"linear6\"][\"name\"] = \"linear\"\n","#pipeline_binary[\"model\"][\"linear6\"][\"neurons\"] = 128\n","#pipeline_binary[\"model\"][\"lrelu6\"] = {}\n","#pipeline_binary[\"model\"][\"lrelu6\"][\"name\"] = \"lrelu\"\n","\n","#pipeline_binary[\"model\"][\"output\"] = {}\n","#pipeline_binary[\"model\"][\"output\"][\"name\"] = \"linear\"\n","#pipeline_binary[\"model\"][\"output\"][\"neurons\"] = 2\n","\n","pipeline_binary[\"optimizer\"] = {}\n","pipeline_binary[\"optimizer\"][\"name\"] = \"adam\"\n","\n","pipeline_binary[\"loss\"] = {}\n","pipeline_binary[\"loss\"][\"type\"] = \"cross_entropy\"\n","pipeline_binary[\"loss\"][\"use_weighted_loss\"] = True\n","pipeline_binary[\"loss\"][\"use_single_neuron\"] = False\n","\n","#---------------------------------------------set up evaluation metrics and parameters------------------------\n","\n","\n","pipeline_binary[\"metrics\"] = {}\n","\n","# accuracy\n","pipeline_binary[\"metrics\"][\"simple_accuracy\"] = {}\n","pipeline_binary[\"metrics\"][\"simple_accuracy\"][\"name\"] = \"accuracy\"\n","pipeline_binary[\"metrics\"][\"simple_accuracy\"][\"type\"] = \"simple\"  \n","\n","pipeline_binary[\"metrics\"][\"balanced_accuracy\"] = {}\n","pipeline_binary[\"metrics\"][\"balanced_accuracy\"][\"name\"] = \"accuracy\"\n","pipeline_binary[\"metrics\"][\"balanced_accuracy\"][\"type\"] = \"balanced\"\n","\n","# precision\n","pipeline_binary[\"metrics\"][\"precision\"] = {}\n","pipeline_binary[\"metrics\"][\"precision\"][\"name\"] = \"precision\"\n","pipeline_binary[\"metrics\"][\"precision\"][\"class_result\"] = \"COVID\"\n","\n","# recall\n","pipeline_binary[\"metrics\"][\"sensitivity\"] = {}\n","pipeline_binary[\"metrics\"][\"sensitivity\"][\"name\"] = \"sensitivity\"\n","pipeline_binary[\"metrics\"][\"sensitivity\"][\"class_result\"]  = \"COVID\"\n","\n","# F1 score\n","pipeline_binary[\"metrics\"][\"f1_score\"] = {}\n","pipeline_binary[\"metrics\"][\"f1_score\"][\"name\"] = \"F1_score\" \n","pipeline_binary[\"metrics\"][\"f1_score\"][\"class_result\"] = \"COVID\"\n","\n","# mcc\n","pipeline_binary[\"metrics\"][\"mcc\"] = {}\n","pipeline_binary[\"metrics\"][\"mcc\"][\"name\"] =\"mcc\" \n","\n","#------------------------------------------------Create Plots --------------------------\n","pipeline_binary[\"plots\"] = [\"CM\", \"LC\", \"MS\"]\n"]},{"cell_type":"code","execution_count":149,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T05:30:33.377553Z","iopub.status.busy":"2023-01-18T05:30:33.376573Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using Device: cpu\n","Unique labels found and their frequencies: \n","Label: COVID, %age: 17.0821\n","Label: NO_COVID, %age: 82.9179\n","Splitting data using Simple Stratified with 70.0-30.0 ratio\n","Split the data successfully. Shape: y_train: (1, 11851, 2) y_valid: (1, 5079, 2)\n"]},{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /home/ahmad/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n","100.0%\n"]},{"name":"stdout","output_type":"stream","text":["\n","Fold No: 1 Model: \n"," Model(\n","  (seq_model): ResNet(\n","    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (relu): ReLU(inplace=True)\n","    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    (layer1): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (layer2): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (3): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (layer3): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (3): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (4): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (5): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (layer4): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","    (fc): Linear(in_features=2048, out_features=2, bias=True)\n","  )\n",")\n","Total trainable parameters: 4098 Total parameters: 23512130\n","\n","Fold No: 1 Training data: Unique labels: \n","Label: 0, %age: 82.9213\n","Label: 1, %age: 17.0787\n","\n","Fold No: 1 Validation data: Unique labels: \n","Label: 0, %age: 82.9100\n","Label: 1, %age: 17.0900\n","CE: using Weighted Loss\n","\n","Fold No: 1 loss: CrossEntropyLoss() weights: tensor([0.1708, 0.8292])\n","\n","Fold No: 1 Optimizer: \n"," Adam (\n","Parameter Group 0\n","    amsgrad: True\n","    betas: (0.9, 0.999)\n","    capturable: False\n","    differentiable: False\n","    eps: 1e-08\n","    foreach: None\n","    fused: False\n","    lr: 0.001\n","    maximize: False\n","    weight_decay: 0\n",")\n","Training :Fold No: 1/1 Epoch:1/20 . . .\n","Evaluated:Fold No: 1/1 Epoch:1/20 Training: Loss: 0.5865, Bal_accu: 0.6868, mcc: 0.316 Validation: Loss: 0.5099, Bal_accu: 0.76, mcc: 0.5422\n","Saving Model of first epoch\n","Training :Fold No: 1/1 Epoch:2/20 . . .\n","Evaluated:Fold No: 1/1 Epoch:2/20 Training: Loss: 0.5342, Bal_accu: 0.7397, mcc: 0.3982 Validation: Loss: 0.5345, Bal_accu: 0.7366, mcc: 0.5226\n","Validaiton Loss score did not improve.\n","Training :Fold No: 1/1 Epoch:3/20 . . .\n","Evaluated:Fold No: 1/1 Epoch:3/20 Training: Loss: 0.4975, Bal_accu: 0.768, mcc: 0.444 Validation: Loss: 0.4356, Bal_accu: 0.8058, mcc: 0.4821\n","Validation Loss score improved. Saving Model.\n","Training :Fold No: 1/1 Epoch:4/20 . . .\n","Evaluated:Fold No: 1/1 Epoch:4/20 Training: Loss: 0.4943, Bal_accu: 0.7642, mcc: 0.435 Validation: Loss: 0.4341, Bal_accu: 0.8017, mcc: 0.5479\n","Validation Loss score improved. Saving Model.\n","Training :Fold No: 1/1 Epoch:5/20 . . .\n","Evaluated:Fold No: 1/1 Epoch:5/20 Training: Loss: 0.4971, Bal_accu: 0.7728, mcc: 0.4574 Validation: Loss: 0.4194, Bal_accu: 0.8152, mcc: 0.5786\n","Validation Loss score improved. Saving Model.\n","Training :Fold No: 1/1 Epoch:6/20 . . .\n","Evaluated:Fold No: 1/1 Epoch:6/20 Training: Loss: 0.457, Bal_accu: 0.7964, mcc: 0.4868 Validation: Loss: 0.4077, Bal_accu: 0.8175, mcc: 0.517\n","Validation Loss score improved. Saving Model.\n","Training :Fold No: 1/1 Epoch:7/20 . . .\n","Evaluated:Fold No: 1/1 Epoch:7/20 Training: Loss: 0.4735, Bal_accu: 0.7827, mcc: 0.4693 Validation: Loss: 0.3921, Bal_accu: 0.8346, mcc: 0.5552\n","Validation Loss score improved. Saving Model.\n","Training :Fold No: 1/1 Epoch:8/20 . . .\n","Evaluated:Fold No: 1/1 Epoch:8/20 Training: Loss: 0.4748, Bal_accu: 0.7785, mcc: 0.4647 Validation: Loss: 0.3943, Bal_accu: 0.8363, mcc: 0.575\n","Validaiton Loss score did not improve.\n","Training :Fold No: 1/1 Epoch:9/20 . . .\n","Evaluated:Fold No: 1/1 Epoch:9/20 Training: Loss: 0.4659, Bal_accu: 0.7842, mcc: 0.4741 Validation: Loss: 0.3948, Bal_accu: 0.8265, mcc: 0.5262\n","Validaiton Loss score did not improve.\n","Training :Fold No: 1/1 Epoch:10/20 . . .\n","Evaluated:Fold No: 1/1 Epoch:10/20 Training: Loss: 0.4454, Bal_accu: 0.7937, mcc: 0.4844 Validation: Loss: 0.3748, Bal_accu: 0.8397, mcc: 0.5327\n","Validation Loss score improved. Saving Model.\n","Training :Fold No: 1/1 Epoch:11/20 . . .\n","Evaluated:Fold No: 1/1 Epoch:11/20 Training: Loss: 0.4625, Bal_accu: 0.7899, mcc: 0.4809 Validation: Loss: 0.3879, Bal_accu: 0.8281, mcc: 0.6107\n","Validaiton Loss score did not improve.\n","Training :Fold No: 1/1 Epoch:12/20 . . .\n","Evaluated:Fold No: 1/1 Epoch:12/20 Training: Loss: 0.4667, Bal_accu: 0.787, mcc: 0.4877 Validation: Loss: 0.3713, Bal_accu: 0.8353, mcc: 0.5734\n","Validation Loss score improved. Saving Model.\n","Training :Fold No: 1/1 Epoch:13/20 . . .\n","Evaluated:Fold No: 1/1 Epoch:13/20 Training: Loss: 0.4451, Bal_accu: 0.7973, mcc: 0.497 Validation: Loss: 0.4007, Bal_accu: 0.8246, mcc: 0.4937\n","Validaiton Loss score did not improve.\n","Training :Fold No: 1/1 Epoch:14/20 . . .\n","Evaluated:Fold No: 1/1 Epoch:14/20 Training: Loss: 0.4498, Bal_accu: 0.7875, mcc: 0.4848 Validation: Loss: 0.3621, Bal_accu: 0.8448, mcc: 0.606\n","Validation Loss score improved. Saving Model.\n","Training :Fold No: 1/1 Epoch:15/20 . . .\n","Evaluated:Fold No: 1/1 Epoch:15/20 Training: Loss: 0.445, Bal_accu: 0.7991, mcc: 0.4999 Validation: Loss: 0.3596, Bal_accu: 0.8484, mcc: 0.6057\n","Validation Loss score improved. Saving Model.\n","Training :Fold No: 1/1 Epoch:16/20 . . .\n","Evaluated:Fold No: 1/1 Epoch:16/20 Training: Loss: 0.4548, Bal_accu: 0.7957, mcc: 0.5001 Validation: Loss: 0.4496, Bal_accu: 0.7883, mcc: 0.6205\n","Validaiton Loss score did not improve.\n","Training :Fold No: 1/1 Epoch:17/20 . . .\n","Evaluated:Fold No: 1/1 Epoch:17/20 Training: Loss: 0.4449, Bal_accu: 0.8017, mcc: 0.5063 Validation: Loss: 0.3528, Bal_accu: 0.8543, mcc: 0.5989\n","Validation Loss score improved. Saving Model.\n","Training :Fold No: 1/1 Epoch:18/20 . . .\n","Evaluated:Fold No: 1/1 Epoch:18/20 Training: Loss: 0.4429, Bal_accu: 0.798, mcc: 0.5046 Validation: Loss: 0.3999, Bal_accu: 0.8261, mcc: 0.4972\n","Validaiton Loss score did not improve.\n","Training :Fold No: 1/1 Epoch:19/20 . . .\n","Evaluated:Fold No: 1/1 Epoch:19/20 Training: Loss: 0.4579, Bal_accu: 0.7945, mcc: 0.4943 Validation: Loss: 0.3818, Bal_accu: 0.8382, mcc: 0.5319\n","Validaiton Loss score did not improve.\n","Training :Fold No: 1/1 Epoch:20/20 . . .\n","Evaluated:Fold No: 1/1 Epoch:20/20 Training: Loss: 0.4286, Bal_accu: 0.803, mcc: 0.5083 Validation: Loss: 0.3707, Bal_accu: 0.8404, mcc: 0.609\n","Validaiton Loss score did not improve.\n","\n","Fold No: 1/1 Best Validation Loss and balanced accuracy recorded on epoch 17: 0.35275003122142695 0.8543008516358201\n","\n","Trained the model for all folds. Shape: train: loss: (1, 20) accu: (1, 20) mcc: (1, 20) valid: loss: (1, 20) accu: (1, 20) mcc: (1, 20) Learning scores: (1, 3, 2, 20)\n","\n","Generating Predictions on training data\n","Generating predicitons for fold no: 0\n","Loading model from /home/ahmad/Documents/TUHH/Semester 3/Intelligent Systems in Medicine/Project/Classification-of-different-dieases-using-ML-and-DL/results/Phase2_results/local_comp/ResNet50/binary/train/0/model.pt\n","Generating Predictions on validation data\n","Generating predicitons for fold no: 0\n","Loading model from /home/ahmad/Documents/TUHH/Semester 3/Intelligent Systems in Medicine/Project/Classification-of-different-dieases-using-ML-and-DL/results/Phase2_results/local_comp/ResNet50/binary/train/0/model.pt\n","\n","Generated predictions on training and validation dataset. Shape: train: labels: (1, 11851, 2) prob: (1, 11851, 2) valid: labels: (1, 5079, 2) prob: (1, 5079, 2)\n","\n","Generating predicitons on complete dataset. . . \n","Generating predicitons for fold no: 0\n","Loading model from /home/ahmad/Documents/TUHH/Semester 3/Intelligent Systems in Medicine/Project/Classification-of-different-dieases-using-ML-and-DL/results/Phase2_results/local_comp/ResNet50/binary/train/0/model.pt\n","\n","Generated predicitons on complete dataset. Shape: Folds: labels: (1, 16930, 2) probs: (1, 16930, 2) Ensembles: labels: (1, 16930, 2) probs: (1, 16930, 2)\n","\n","Evaluating Metrics on training data . . . \n","Processing: Fold No: 0 Metric: simple_accuracy\n","Processing: Fold No: 0 Metric: balanced_accuracy\n","Processing: Fold No: 0 Metric: precision\n","Processing: Fold No: 0 Metric: sensitivity\n","Processing: Fold No: 0 Metric: f1_score\n","Processing: Fold No: 0 Metric: mcc\n","\n","Results\n","Fold No: 0 Metric: simple_accuracy  Score: 0.8522999882698059 \n","Fold No: 0 Metric: balanced_accuracy  Score: 0.8445000052452087 \n","Fold No: 0 Metric: precision  Score: 0.5443000197410583 \n","Fold No: 0 Metric: sensitivity  Score: 0.8324999809265137 \n","Fold No: 0 Metric: f1_score  Score: 0.6582000255584717 \n","Fold No: 0 Metric: mcc  Score: 0.5900999903678894 \n","Evaluated Metrics on the training data successfully. Shape:(1, 6)\n","\n","Evaluating Metrics on validation data . . . \n","Processing: Fold No: 0 Metric: simple_accuracy\n","Processing: Fold No: 0 Metric: balanced_accuracy\n","Processing: Fold No: 0 Metric: precision\n","Processing: Fold No: 0 Metric: sensitivity\n","Processing: Fold No: 0 Metric: f1_score\n","Processing: Fold No: 0 Metric: mcc\n","\n","Results\n","Fold No: 0 Metric: simple_accuracy  Score: 0.8565000295639038 \n","Fold No: 0 Metric: balanced_accuracy  Score: 0.8521999716758728 \n","Fold No: 0 Metric: precision  Score: 0.552299976348877 \n","Fold No: 0 Metric: sensitivity  Score: 0.8456000089645386 \n","Fold No: 0 Metric: f1_score  Score: 0.6682000160217285 \n","Fold No: 0 Metric: mcc  Score: 0.6032000184059143 \n","Evaluated Metrics on the validation data successfully. Shape: (1, 6)\n","\n","Evaluating Metrics on complete data . . . \n","Processing: Fold No: 0 Metric: simple_accuracy\n","Processing: Fold No: 0 Metric: balanced_accuracy\n","Processing: Fold No: 0 Metric: precision\n","Processing: Fold No: 0 Metric: sensitivity\n","Processing: Fold No: 0 Metric: f1_score\n","Processing: Fold No: 0 Metric: mcc\n","\n","Results\n","Fold No: 0 Metric: simple_accuracy  Score: 0.853600025177002 \n","Fold No: 0 Metric: balanced_accuracy  Score: 0.8468000292778015 \n","Fold No: 0 Metric: precision  Score: 0.5467000007629395 \n","Fold No: 0 Metric: sensitivity  Score: 0.8363999724388123 \n","Fold No: 0 Metric: f1_score  Score: 0.6611999869346619 \n","Fold No: 0 Metric: mcc  Score: 0.5940999984741211 \n","Evaluated Metrics on the complete dataset successfully. Shape:(1, 6)\n","\n","Evaluating Metrics on complete dataset using ensembles . . . \n","Processing: Fold No: 0 Metric: simple_accuracy\n","Processing: Fold No: 0 Metric: balanced_accuracy\n","Processing: Fold No: 0 Metric: precision\n","Processing: Fold No: 0 Metric: sensitivity\n","Processing: Fold No: 0 Metric: f1_score\n","Processing: Fold No: 0 Metric: mcc\n","\n","Results\n","Fold No: 0 Metric: simple_accuracy  Score: 0.853600025177002 \n","Fold No: 0 Metric: balanced_accuracy  Score: 0.8468000292778015 \n","Fold No: 0 Metric: precision  Score: 0.5467000007629395 \n","Fold No: 0 Metric: sensitivity  Score: 0.8363999724388123 \n","Fold No: 0 Metric: f1_score  Score: 0.6611999869346619 \n","Fold No: 0 Metric: mcc  Score: 0.5940999984741211 \n","Evaluated Metrics on the complete dataset using ensemble successfully. Shape:(1, 6)\n","\n","Creating Plots for training data . . .\n","Creating Plot: CM Fold No: 0\n","Creating Plot: LC Fold No: 0\n","Creating Plot: MS Fold No: 0\n","Created Plots for training data\n","\n","Creating Plots for validation data\n","Creating Plot: CM Fold No: 0\n","Warning: key'learning_curves' provided in dict 'plots' but metric score not found. Skipping ploting learning curves\n","Creating Plot: MS Fold No: 0\n","Created Plots for validation data\n","\n","Creating Plots for complete dataset\n","Creating Plot: CM Fold No: 0\n","Warning: key'learning_curves' provided in dict 'plots' but metric score not found. Skipping ploting learning curves\n","Creating Plot: MS Fold No: 0\n","Created Plots for complete dataset\n","\n","Creating Plots for ensemble dataset\n","Creating Plot: CM Fold No: 0\n","Warning: key'learning_curves' provided in dict 'plots' but metric score not found. Skipping ploting learning curves\n","Creating Plot: MS Fold No: 0\n","Created Plots for ensemble data\n","\n","saving pipeline dictionary to json\n","\n","Generating text file for training data\n","Processing Fold No: 0\n","\n","Generating text file for validation data\n","Processing Fold No: 0\n","\n","Generating text file for complete data\n","Processing Fold No: 0\n","\n","Generating text file for ensemble data\n","Processing Fold No: 0\n","\n","Saving training results \n","\n","Saving validation results \n","\n","Saving complete data results \n","\n","Saving ensemble results \n","\n","Training Completed\n","\n","Time taken to train the model :  4:30:22.456285\n"]}],"source":["start = time.time()\n","CNN_training(pipeline_binary)\n","end = time.time()\n","print(\"Time taken to train the model : \", datetime.timedelta(seconds=end-start))"]},{"cell_type":"markdown","metadata":{},"source":["## test data "]},{"cell_type":"code","execution_count":150,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Created directory /home/ahmad/Documents/TUHH/Semester 3/Intelligent Systems in Medicine/Project/Classification-of-different-dieases-using-ML-and-DL/results/Phase2_results/local_comp/ResNet50/binary/test\n","Using Device: cpu\n","\n","Generating labels for the data . . . \n","Generating predicitons for fold no: 0\n","Loading model from /home/ahmad/Documents/TUHH/Semester 3/Intelligent Systems in Medicine/Project/Classification-of-different-dieases-using-ML-and-DL/results/Phase2_results/local_comp/ResNet50/binary/train/0/model.pt\n","Generated labels for the data successfully. Shape: y_pred: (1, 4235, 2) y_pred_probs: (1, 4235, 2) y_pred_en: (1, 4235, 2) y_pred_probs_en: (1, 4235, 2)\n","\n","saving pipeline dictionary to json\n","\n","Generating text file for the data\n","Processing Fold No: 0\n","\n","Generating text file for the data ensemble\n","Processing Fold No: 0\n","\n","Generated Predictions Successfully\n","Time taken to generate predictions on test data :  0:02:47.010450\n"]}],"source":["path_to_json = os.path.join(path_to_binary_results, \"train\", \"training_pipeline.json\")\n","save_path = os.path.join(path_to_binary_results, \"test\")\n","\n","start = time.time()\n","CNN_prediction(path_to_test_data, path_to_json, save_path)\n","end = time.time()\n","print(\"Time taken to generate predictions on test data : \", datetime.timedelta(seconds=end-start))"]},{"cell_type":"markdown","metadata":{},"source":["## noisy test data "]},{"cell_type":"code","execution_count":151,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Created directory /home/ahmad/Documents/TUHH/Semester 3/Intelligent Systems in Medicine/Project/Classification-of-different-dieases-using-ML-and-DL/results/Phase2_results/local_comp/ResNet50/binary/noisy_test\n","Using Device: cpu\n","\n","Generating labels for the data . . . \n","Generating predicitons for fold no: 0\n","Loading model from /home/ahmad/Documents/TUHH/Semester 3/Intelligent Systems in Medicine/Project/Classification-of-different-dieases-using-ML-and-DL/results/Phase2_results/local_comp/ResNet50/binary/train/0/model.pt\n","Generated labels for the data successfully. Shape: y_pred: (1, 4235, 2) y_pred_probs: (1, 4235, 2) y_pred_en: (1, 4235, 2) y_pred_probs_en: (1, 4235, 2)\n","\n","saving pipeline dictionary to json\n","\n","Generating text file for the data\n","Processing Fold No: 0\n","\n","Generating text file for the data ensemble\n","Processing Fold No: 0\n","\n","Generated Predictions Successfully\n","Time taken to generate predictions on noisy test data :  0:02:47.988636\n"]}],"source":["path_to_json = os.path.join(path_to_binary_results, \"train\", \"training_pipeline.json\")\n","save_path = os.path.join(path_to_binary_results, \"noisy_test\")\n","\n","start = time.time()\n","CNN_prediction(path_to_noisy_test_data, path_to_json, save_path)\n","end = time.time()\n","print(\"Time taken to generate predictions on noisy test data : \", datetime.timedelta(seconds=end-start))"]},{"cell_type":"markdown","metadata":{},"source":["# Multiclass Classification"]},{"cell_type":"markdown","metadata":{},"source":["## Train "]},{"cell_type":"code","execution_count":152,"metadata":{"trusted":true},"outputs":[],"source":["pipeline_multi = {}\n","pipeline_multi[\"path_to_results\"] = os.path.join(path_to_multiclass_results, \"train\")\n","\n","pipeline_multi[\"batch_size\"] = 32\n","pipeline_multi[\"num_epochs\"] = 20\n","\n","# for model\n","pipeline_multi[\"input_shape\"] = (224, 224) \n","\n","# path to folder containing images \n","pipeline_multi[\"path_to_images\"] = path_to_train_data\n","# path to labels.txt\n","pipeline_multi[\"path_to_labels\"] = path_to_multi_labels\n","\n","# split data\n","pipeline_multi[\"split_type\"] =\"simpleStratified\" #\"kfoldStratified\" #\"simpleStratified\" #\"simple\", \"simpleStratified\", \"kfold\", \"kfoldStratified\"\n","#pipeline_multi[\"num_folds\"] = 2\n","pipeline_multi[\"test_size\"] = 0.3\n","\n","# names of the class\n","pipeline_multi[\"classes\"] = np.array([\"Normal\", \"COVID\", \"pneumonia\", \"Lung_Opacity\"])\n","pipeline_multi[\"use_pretrained_model\"]= True\n","pipeline_multi[\"read_img_color\"] = \"rgb\"\n","\n","# ---------------------------------set up data preprocessing methods and parameters------------------------------------\n","pipeline_multi[\"data_preprocessing\"] = {}\n","pipeline_multi[\"data_preprocessing\"][\"train\"] = OrderedDict()\n","pipeline_multi[\"data_preprocessing\"][\"train\"][\"normalize_0_1\"] = {}\n","pipeline_multi[\"data_preprocessing\"][\"train\"][\"normalize_0_1\"][\"name\"] = \"normalize\"\n","pipeline_multi[\"data_preprocessing\"][\"train\"][\"normalize_0_1\"][\"mean\"] = 0\n","pipeline_multi[\"data_preprocessing\"][\"train\"][\"normalize_0_1\"][\"std\"] = 255\n","\n","pipeline_multi[\"data_preprocessing\"][\"train\"][\"random_rotation\"] = {}\n","pipeline_multi[\"data_preprocessing\"][\"train\"][\"random_rotation\"][\"name\"] =  \"random_rotation\"\n","pipeline_multi[\"data_preprocessing\"][\"train\"][\"random_rotation\"][\"degrees\"] = 90\n","pipeline_multi[\"data_preprocessing\"][\"train\"][\"random_rotation\"][\"expand\"] = True\n","\n","pipeline_multi[\"data_preprocessing\"][\"train\"][\"random_crop\"] = {}\n","pipeline_multi[\"data_preprocessing\"][\"train\"][\"random_crop\"][\"name\"] = \"random_resized_crop\"\n","pipeline_multi[\"data_preprocessing\"][\"train\"][\"random_crop\"][\"output_shape\"] = 224\n","\n","pipeline_multi[\"data_preprocessing\"][\"train\"][\"horizontal_flip\"] = {}\n","pipeline_multi[\"data_preprocessing\"][\"train\"][\"horizontal_flip\"][\"name\"] = \"random_horizontal_flip\"\n","\n","pipeline_multi[\"data_preprocessing\"][\"train\"][\"vertical_flip\"] ={}\n","pipeline_multi[\"data_preprocessing\"][\"train\"][\"vertical_flip\"][\"name\"] = \"random_vertical_flip\"\n","\n","pipeline_multi[\"data_preprocessing\"][\"train\"][\"normalize_model_values\"] = {}\n","pipeline_multi[\"data_preprocessing\"][\"train\"][\"normalize_model_values\"][\"name\"] = \"normalize\"\n","pipeline_multi[\"data_preprocessing\"][\"train\"][\"normalize_model_values\"][\"mean\"] = [0.485, 0.456, 0.406]\n","pipeline_multi[\"data_preprocessing\"][\"train\"][\"normalize_model_values\"][\"std\"] = [0.229, 0.224, 0.225]\n","\n","\n","pipeline_multi[\"data_preprocessing\"][\"valid\"] = OrderedDict()\n","\n","pipeline_multi[\"data_preprocessing\"][\"valid\"][\"normalize_0_1\"] = {}\n","pipeline_multi[\"data_preprocessing\"][\"valid\"][\"normalize_0_1\"][\"name\"] = \"normalize\"\n","pipeline_multi[\"data_preprocessing\"][\"valid\"][\"normalize_0_1\"][\"mean\"] = 0\n","pipeline_multi[\"data_preprocessing\"][\"valid\"][\"normalize_0_1\"][\"std\"] = 255\n","\n","pipeline_multi[\"data_preprocessing\"][\"valid\"][\"center_crop\"] = {}\n","pipeline_multi[\"data_preprocessing\"][\"valid\"][\"center_crop\"][\"name\"] = \"center_crop\"\n","pipeline_multi[\"data_preprocessing\"][\"valid\"][\"center_crop\"][\"output_shape\"] = (224,224)\n","\n","pipeline_multi[\"data_preprocessing\"][\"valid\"][\"normalize_model_values\"] = {}\n","pipeline_multi[\"data_preprocessing\"][\"valid\"][\"normalize_model_values\"][\"name\"] = \"normalize\"\n","pipeline_multi[\"data_preprocessing\"][\"valid\"][\"normalize_model_values\"][\"mean\"] = [0.485, 0.456, 0.406]\n","pipeline_multi[\"data_preprocessing\"][\"valid\"][\"normalize_model_values\"][\"std\"] = [0.229, 0.224, 0.225]\n","\n","\n","\n","# ---------------------------------set up networks and parameters------------------------------------\n","\n","pipeline_multi[\"model\"] = OrderedDict()\n","pipeline_multi[\"model\"][\"resnet50\"] = {}\n","pipeline_multi[\"model\"][\"resnet50\"][\"name\"] = \"resnet50\"\n","pipeline_multi[\"model\"][\"resnet50\"][\"num_output_neurons\"] = 4\n","#pipeline_multi[\"model\"][\"mobile_net\"][\"num_layers_to_train\"] = 4\n","\n","#pipeline_multi[\"model\"][\"conv1\"] = {}\n","#pipeline_multi[\"model\"][\"conv1\"][\"name\"] = \"conv\"\n","#pipeline_multi[\"model\"][\"conv1\"][\"number_of_kernels\"] = 32\n","#pipeline_multi[\"model\"][\"conv1\"][\"kernel_size\"] = 7\n","#pipeline_multi[\"model\"][\"lrelu1\"] = {}\n","#pipeline_multi[\"model\"][\"lrelu1\"][\"name\"] = \"lrelu\"\n","#pipeline_multi[\"model\"][\"maxpool1\"] = {}\n","#pipeline_multi[\"model\"][\"maxpool1\"][\"name\"] = \"max_pool\"\n","#pipeline_multi[\"model\"][\"maxpool1\"][\"kernel_size\"] = 2\n","\n","\n","#pipeline_multi[\"model\"][\"conv2\"] = {}\n","#pipeline_multi[\"model\"][\"conv2\"][\"name\"] = \"conv\"\n","#pipeline_multi[\"model\"][\"conv2\"][\"number_of_kernels\"] = 64\n","#pipeline_multi[\"model\"][\"conv2\"][\"kernel_size\"] = 5\n","#pipeline_multi[\"model\"][\"lrelu2\"] = {}\n","#pipeline_multi[\"model\"][\"lrelu2\"][\"name\"] = \"lrelu\"\n","#pipeline_multi[\"model\"][\"maxpool2\"] = {}\n","#pipeline_multi[\"model\"][\"maxpool2\"][\"name\"] = \"max_pool\"\n","#pipeline_multi[\"model\"][\"maxpool2\"][\"kernel_size\"] = 2\n","\n","\n","#pipeline_multi[\"model\"][\"conv3\"] = {}\n","#pipeline_multi[\"model\"][\"conv3\"][\"name\"] = \"conv\"\n","#pipeline_multi[\"model\"][\"conv3\"][\"number_of_kernels\"] = 128\n","#pipeline_multi[\"model\"][\"conv3\"][\"kernel_size\"] = 3\n","#pipeline_multi[\"model\"][\"lrelu3\"] = {}\n","#pipeline_multi[\"model\"][\"lrelu3\"][\"name\"] = \"lrelu\"\n","#pipeline_multi[\"model\"][\"maxpool3\"] = {}\n","#pipeline_multi[\"model\"][\"maxpool3\"][\"name\"] = \"max_pool\"\n","#pipeline_multi[\"model\"][\"maxpool3\"][\"kernel_size\"] = 3\n","\n","#pipeline_multi[\"model\"][\"flatten4\"] = {}\n","#pipeline_multi[\"model\"][\"flatten4\"][\"name\"] = \"flatten\"\n","\n","#pipeline_multi[\"model\"][\"linear5\"] = {}\n","#pipeline_multi[\"model\"][\"linear5\"][\"name\"] = \"linear\"\n","#pipeline_multi[\"model\"][\"linear5\"][\"neurons\"] = 256\n","#pipeline_multi[\"model\"][\"lrelu5\"] = {}\n","#pipeline_multi[\"model\"][\"lrelu5\"][\"name\"] = \"lrelu\"\n","\n","#pipeline_multi[\"model\"][\"linear6\"] = {}\n","#pipeline_multi[\"model\"][\"linear6\"][\"name\"] = \"linear\"\n","#pipeline_multi[\"model\"][\"linear6\"][\"neurons\"] = 128\n","#pipeline_multi[\"model\"][\"lrelu6\"] = {}\n","#pipeline_multi[\"model\"][\"lrelu6\"][\"name\"] = \"lrelu\"\n","\n","#pipeline_multi[\"model\"][\"output\"] = {}\n","#pipeline_multi[\"model\"][\"output\"][\"name\"] = \"linear\"\n","#pipeline_multi[\"model\"][\"output\"][\"neurons\"] = 4\n","\n","pipeline_multi[\"optimizer\"] = {}\n","pipeline_multi[\"optimizer\"][\"name\"] = \"adam\"\n","\n","pipeline_multi[\"loss\"] = {}\n","pipeline_multi[\"loss\"][\"type\"] = \"cross_entropy\"\n","pipeline_multi[\"loss\"][\"use_weighted_loss\"] = True\n","pipeline_multi[\"loss\"][\"use_single_neuron\"] = False\n","\n","\n","\n","#---------------------------------------------set up evaluation metrics and parameters------------------------\n","\n","\n","pipeline_multi[\"metrics\"] = {}\n","\n","# accuracy\n","pipeline_multi[\"metrics\"][\"simple_accuracy\"] = {}\n","pipeline_multi[\"metrics\"][\"simple_accuracy\"][\"name\"] = \"accuracy\"\n","pipeline_multi[\"metrics\"][\"simple_accuracy\"][\"type\"] = \"simple\"  \n","\n","pipeline_multi[\"metrics\"][\"balanced_accuracy\"] = {}\n","pipeline_multi[\"metrics\"][\"balanced_accuracy\"][\"name\"] = \"accuracy\"\n","pipeline_multi[\"metrics\"][\"balanced_accuracy\"][\"type\"] = \"balanced\"\n","\n","# precision\n","pipeline_multi[\"metrics\"][\"precision\"] = {}\n","pipeline_multi[\"metrics\"][\"precision\"][\"name\"] = \"precision\"\n","pipeline_multi[\"metrics\"][\"precision\"][\"class_result\"] = \"COVID\"\n","\n","# recall\n","pipeline_multi[\"metrics\"][\"sensitivity\"] = {}\n","pipeline_multi[\"metrics\"][\"sensitivity\"][\"name\"] = \"sensitivity\"\n","pipeline_multi[\"metrics\"][\"sensitivity\"][\"class_result\"]  = \"COVID\"\n","\n","# F1 score\n","pipeline_multi[\"metrics\"][\"f1_score\"] = {}\n","pipeline_multi[\"metrics\"][\"f1_score\"][\"name\"] = \"F1_score\" \n","pipeline_multi[\"metrics\"][\"f1_score\"][\"class_result\"] = \"COVID\"\n","\n","# mcc\n","pipeline_multi[\"metrics\"][\"mcc\"] = {}\n","pipeline_multi[\"metrics\"][\"mcc\"][\"name\"] =\"mcc\" \n","\n","#------------------------------------------------Create Plots --------------------------\n","pipeline_multi[\"plots\"] = [\"CM\", \"LC\", \"MS\"]\n"]},{"cell_type":"code","execution_count":153,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using Device: cpu\n","Unique labels found and their frequencies: \n","Label: COVID, %age: 17.0821\n","Label: Lung_Opacity, %age: 28.4052\n","Label: Normal, %age: 48.1571\n","Label: pneumonia, %age: 6.3556\n","Splitting data using Simple Stratified with 70.0-30.0 ratio\n","Split the data successfully. Shape: y_train: (1, 11851, 2) y_valid: (1, 5079, 2)\n","\n","Fold No: 1 Model: \n"," Model(\n","  (seq_model): ResNet(\n","    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (relu): ReLU(inplace=True)\n","    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    (layer1): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (layer2): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (3): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (layer3): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (3): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (4): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (5): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (layer4): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","    (fc): Linear(in_features=2048, out_features=4, bias=True)\n","  )\n",")\n","Total trainable parameters: 8196 Total parameters: 23516228\n","\n","Fold No: 1 Training data: Unique labels: \n","Label: 0, %age: 48.1563\n","Label: 1, %age: 17.0872\n","Label: 2, %age: 6.3539\n","Label: 3, %age: 28.4027\n","\n","Fold No: 1 Validation data: Unique labels: \n","Label: 0, %age: 48.1591\n","Label: 1, %age: 17.0703\n","Label: 2, %age: 6.3595\n","Label: 3, %age: 28.4111\n","CE: using Weighted Loss\n","\n","Fold No: 1 loss: CrossEntropyLoss() weights: tensor([0.5184, 0.8291, 0.9365, 0.7160])\n","\n","Fold No: 1 Optimizer: \n"," Adam (\n","Parameter Group 0\n","    amsgrad: True\n","    betas: (0.9, 0.999)\n","    capturable: False\n","    differentiable: False\n","    eps: 1e-08\n","    foreach: None\n","    fused: False\n","    lr: 0.001\n","    maximize: False\n","    weight_decay: 0\n",")\n","Training :Fold No: 1/1 Epoch:1/20 . . .\n","Evaluated:Fold No: 1/1 Epoch:1/20 Training: Loss: 0.9122, Bal_accu: 0.5642, mcc: 0.4608 Validation: Loss: 0.6762, Bal_accu: 0.7636, mcc: 0.6275\n","Saving Model of first epoch\n","Training :Fold No: 1/1 Epoch:2/20 . . .\n","Evaluated:Fold No: 1/1 Epoch:2/20 Training: Loss: 0.7786, Bal_accu: 0.6778, mcc: 0.551 Validation: Loss: 0.5923, Bal_accu: 0.7692, mcc: 0.6798\n","Validation Loss score improved. Saving Model.\n","Training :Fold No: 1/1 Epoch:3/20 . . .\n","Evaluated:Fold No: 1/1 Epoch:3/20 Training: Loss: 0.7651, Bal_accu: 0.6847, mcc: 0.5605 Validation: Loss: 0.5846, Bal_accu: 0.7783, mcc: 0.6724\n","Validation Loss score improved. Saving Model.\n","Training :Fold No: 1/1 Epoch:4/20 . . .\n","Evaluated:Fold No: 1/1 Epoch:4/20 Training: Loss: 0.7387, Bal_accu: 0.6983, mcc: 0.5773 Validation: Loss: 0.5943, Bal_accu: 0.7372, mcc: 0.6647\n","Validaiton Loss score did not improve.\n","Training :Fold No: 1/1 Epoch:5/20 . . .\n","Evaluated:Fold No: 1/1 Epoch:5/20 Training: Loss: 0.7435, Bal_accu: 0.6966, mcc: 0.5742 Validation: Loss: 0.7077, Bal_accu: 0.6526, mcc: 0.6134\n","Validaiton Loss score did not improve.\n","Training :Fold No: 1/1 Epoch:6/20 . . .\n","Evaluated:Fold No: 1/1 Epoch:6/20 Training: Loss: 0.7231, Bal_accu: 0.7028, mcc: 0.5849 Validation: Loss: 0.577, Bal_accu: 0.7738, mcc: 0.6746\n","Validation Loss score improved. Saving Model.\n","Training :Fold No: 1/1 Epoch:7/20 . . .\n","Evaluated:Fold No: 1/1 Epoch:7/20 Training: Loss: 0.7072, Bal_accu: 0.7231, mcc: 0.6028 Validation: Loss: 0.5685, Bal_accu: 0.7479, mcc: 0.676\n","Validation Loss score improved. Saving Model.\n","Training :Fold No: 1/1 Epoch:8/20 . . .\n","Evaluated:Fold No: 1/1 Epoch:8/20 Training: Loss: 0.7007, Bal_accu: 0.7183, mcc: 0.5941 Validation: Loss: 0.5527, Bal_accu: 0.7768, mcc: 0.6891\n","Validation Loss score improved. Saving Model.\n","Training :Fold No: 1/1 Epoch:9/20 . . .\n","Evaluated:Fold No: 1/1 Epoch:9/20 Training: Loss: 0.7081, Bal_accu: 0.7227, mcc: 0.5991 Validation: Loss: 0.5355, Bal_accu: 0.7755, mcc: 0.6933\n","Validation Loss score improved. Saving Model.\n","Training :Fold No: 1/1 Epoch:10/20 . . .\n","Evaluated:Fold No: 1/1 Epoch:10/20 Training: Loss: 0.706, Bal_accu: 0.7206, mcc: 0.5919 Validation: Loss: 0.5261, Bal_accu: 0.7927, mcc: 0.7084\n","Validation Loss score improved. Saving Model.\n","Training :Fold No: 1/1 Epoch:11/20 . . .\n","Evaluated:Fold No: 1/1 Epoch:11/20 Training: Loss: 0.6973, Bal_accu: 0.7205, mcc: 0.5964 Validation: Loss: 0.5526, Bal_accu: 0.7613, mcc: 0.6863\n","Validaiton Loss score did not improve.\n","Training :Fold No: 1/1 Epoch:12/20 . . .\n","Evaluated:Fold No: 1/1 Epoch:12/20 Training: Loss: 0.7002, Bal_accu: 0.7223, mcc: 0.6076 Validation: Loss: 0.5565, Bal_accu: 0.7454, mcc: 0.6851\n","Validaiton Loss score did not improve.\n","Training :Fold No: 1/1 Epoch:13/20 . . .\n","Evaluated:Fold No: 1/1 Epoch:13/20 Training: Loss: 0.6844, Bal_accu: 0.736, mcc: 0.6134 Validation: Loss: 0.6239, Bal_accu: 0.7442, mcc: 0.6232\n","Validaiton Loss score did not improve.\n","Training :Fold No: 1/1 Epoch:14/20 . . .\n","Evaluated:Fold No: 1/1 Epoch:14/20 Training: Loss: 0.6768, Bal_accu: 0.7334, mcc: 0.611 Validation: Loss: 0.5553, Bal_accu: 0.744, mcc: 0.6872\n","Validaiton Loss score did not improve.\n","Training :Fold No: 1/1 Epoch:15/20 . . .\n","Evaluated:Fold No: 1/1 Epoch:15/20 Training: Loss: 0.6602, Bal_accu: 0.7414, mcc: 0.6233 Validation: Loss: 0.5584, Bal_accu: 0.7559, mcc: 0.6818\n","Validaiton Loss score did not improve.\n","Training :Fold No: 1/1 Epoch:16/20 . . .\n","Evaluated:Fold No: 1/1 Epoch:16/20 Training: Loss: 0.6635, Bal_accu: 0.7457, mcc: 0.6236 Validation: Loss: 0.5732, Bal_accu: 0.7506, mcc: 0.6875\n","Validaiton Loss score did not improve.\n","Training :Fold No: 1/1 Epoch:17/20 . . .\n","Evaluated:Fold No: 1/1 Epoch:17/20 Training: Loss: 0.6615, Bal_accu: 0.737, mcc: 0.6259 Validation: Loss: 0.5288, Bal_accu: 0.7657, mcc: 0.7085\n","Validaiton Loss score did not improve.\n","Training :Fold No: 1/1 Epoch:18/20 . . .\n","Evaluated:Fold No: 1/1 Epoch:18/20 Training: Loss: 0.6809, Bal_accu: 0.7288, mcc: 0.6128 Validation: Loss: 0.5146, Bal_accu: 0.8257, mcc: 0.7124\n","Validation Loss score improved. Saving Model.\n","Training :Fold No: 1/1 Epoch:19/20 . . .\n","Evaluated:Fold No: 1/1 Epoch:19/20 Training: Loss: 0.6653, Bal_accu: 0.7339, mcc: 0.6179 Validation: Loss: 0.4989, Bal_accu: 0.8115, mcc: 0.7111\n","Validation Loss score improved. Saving Model.\n","Training :Fold No: 1/1 Epoch:20/20 . . .\n"]}],"source":["start =time.time()\n","CNN_training(pipeline_multi)\n","end = time.time()\n","print(\"Time taken to train the model : \", datetime.timedelta(seconds=end-start))"]},{"cell_type":"markdown","metadata":{},"source":["## Test data "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["path_to_json = os.path.join(path_to_multiclass_results, \"train\", \"training_pipeline.json\")\n","save_path = os.path.join(path_to_multiclass_results, \"test\")\n","\n","start = time.time()\n","CNN_prediction(path_to_test_data, path_to_json, save_path)\n","end = time.time()\n","print(\"Time taken to generate predicitons on test data: \",  datetime.timedelta(seconds=end-start))"]},{"cell_type":"markdown","metadata":{},"source":["## Noisy data "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["save_path = os.path.join(path_to_multiclass_results, \"noisy_test\")\n","\n","start = time.time()\n","CNN_prediction(path_to_noisy_test_data, path_to_json, save_path)\n","end = time.time()\n","print(\"Time taken to generate predicitons on noisy test data: \", datetime.timedelta(seconds=end-start))"]},{"cell_type":"markdown","metadata":{},"source":["# Create zip file"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["raise ValueError(\"PROCESSING COMPLETED\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["cmd = \"tar -zcvf \" + path_to_results + \".tar.gz \" + path_to_results\n","os.system(cmd)"]},{"cell_type":"markdown","metadata":{},"source":["# Joel code"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["raise ValueError(\"Entering Joel Code\")"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["import os\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","import torch\n","from skimage import io, transform\n","\n","from PIL import Image\n","\n","class2id = test_count = {'Normal': 0, 'COVID': 1, 'pneumonia': 2, 'Lung_Opacity': 3}\n","\n","id2class = {0: 'Normal', 1: 'COVID', 2: 'pneumonia', 3:'Lung_Opacity'}\n","\n","class COVIDxDataset(Dataset):\n","    def __init__(self, txt_frame_file, images_path, transform=None):\n","        \"\"\"\n","        Args:\n","            txt_frame_file (string): Path to the txt files with labels.\n","            images_path (string): Directory with all the images.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        self.covidx_frame = pd.read_csv(txt_frame_file, delim_whitespace=True) \n","        \n","        #self.landmarks_frame = pd.read_csv(csv_file)\n","        self.root_dir = images_path\n","        self.transform = transform\n","        self.class2id = test_count = {'Normal': 0, 'COVID': 1, 'pneumonia': 2, 'Lung_Opacity': 3}\n","        self.id2class = {0: 'Normal', 1: 'COVID', 2: 'pneumonia', 3:'Lung_Opacity'}\n","        \n","        \n","    def pil_loader(self, path):\n","        with open(path, 'rb') as f:\n","            img = Image.open(f)\n","            return img.convert('RGB')\n","        \n","    \n","    def __len__(self):\n","        return len(self.covidx_frame)\n","    \n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        image_path = os.path.join(self.root_dir,\n","                                self.covidx_frame.iloc[idx, 1])\n","        #print(img_name)\n","        \n","        image = self.pil_loader(image_path)\n","        #image = io.imread(img_name)\n","        \n","        \n","        label = self.covidx_frame.iloc[idx, 2]\n","        #landmarks = np.array([landmarks])\n","        #landmarks = landmarks.astype('float').reshape(-1, 2)\n","        #sample = {'image': image, 'label': label}\n","\n","        if self.transform is not None:\n","            image = self.transform(image)\n","            \n","        sample = (image, self.class2id[label])\n","\n","        return sample\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import torchvision\n","from torchvision import datasets, models, transforms, utils\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import copy\n","\n","import math\n","\n","from torch.utils.data import random_split"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["model_name = 'resnet'\n","batch_size = 32\n","num_epochs = 10\n","feature_extract = True\n","\n","#TODO: get automatically these parameters\n","num_classes = 4"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def train_model(model, dataloaders, criterion, optimizer, num_epochs = 25, is_inception= False):\n","    since = time.time()\n","    \n","    val_acc_history = []\n","    \n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","    \n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs -1))\n","        \n","        #Each epoch as a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()\n","            else:\n","                model.eval()\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","            \n","            # Iterate over data\n","            for inputs, labels in dataloaders[phase]:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","                \n","                #zero the parameter gradients\n","                optimizer.zero_grad()\n","                \n","                #forward\n","                # track history\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    \n","                    if is_inception and phase == 'train':\n","                        outputs, aux_outputs = model(inputs)\n","                        loss1 = criterion(outputs, labels)\n","                        loss2 = criterion(aux_outputs, labels)\n","                        loss = loss1 + 0.4*loss2\n","                        \n","                    else:\n","                        outputs = model(inputs)\n","                        loss = criterion(outputs, labels)\n","                        \n","                    _, preds = torch.max(outputs, 1)\n","                    \n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                #statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","                \n","            epoch_loss = running_loss /len(dataloaders[phase].dataset)\n","            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n","            \n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n","            \n","            # deep copy the model\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","            if phase == 'val':\n","                val_acc_history.append(epoch_acc)\n","            \n","        print()\n","        \n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:.4f}'.format(best_acc))\n","    \n","    #load the best model\n","    model.load_state_dict(best_model_wts)\n","    return model, val_acc_history"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["# set model parameters\n","def set_parameter_requires_grad(model, feature_extracting):\n","    if feature_extracting:\n","        for param in model.parameters():\n","            param.requires_grad = False"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n","    # Initialize these variables which will be set in this if statement. Each of these\n","    #   variables is model specific.\n","    model_ft = None\n","    input_size = 0\n","\n","    if model_name == \"resnet\":\n","        \"\"\" Resnet18\n","        \"\"\"\n","        model_ft = models.resnet18(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        num_ftrs = model_ft.fc.in_features\n","        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n","        input_size = 224\n","\n","    elif model_name == \"alexnet\":\n","        \"\"\" Alexnet\n","        \"\"\"\n","        model_ft = models.alexnet(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        num_ftrs = model_ft.classifier[6].in_features\n","        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n","        input_size = 224\n","\n","    elif model_name == \"vgg\":\n","        \"\"\" VGG11_bn\n","        \"\"\"\n","        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        num_ftrs = model_ft.classifier[6].in_features\n","        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n","        input_size = 224\n","\n","    elif model_name == \"squeezenet\":\n","        \"\"\" Squeezenet\n","        \"\"\"\n","        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n","        model_ft.num_classes = num_classes\n","        input_size = 224\n","\n","    elif model_name == \"densenet\":\n","        \"\"\" Densenet\n","        \"\"\"\n","        model_ft = models.densenet121(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        num_ftrs = model_ft.classifier.in_features\n","        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n","        input_size = 224\n","\n","    elif model_name == \"inception\":\n","        \"\"\" Inception v3\n","        Be careful, expects (299,299) sized images and has auxiliary output\n","        \"\"\"\n","        model_ft = models.inception_v3(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        # Handle the auxilary net\n","        num_ftrs = model_ft.AuxLogits.fc.in_features\n","        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n","        # Handle the primary net\n","        num_ftrs = model_ft.fc.in_features\n","        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n","        input_size = 299\n","\n","    else:\n","        print(\"Invalid model name, exiting...\")\n","        exit()\n","\n","    return model_ft, input_size"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["# Initialize the model for this run\n","#model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n","\n","# Print the model we just instantiated\n","#print(model_ft)"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["# Data augmentation and normalization for training\n","# Just normalization for validation\n","data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.RandomResizedCrop(input_size),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Resize(input_size),\n","        transforms.CenterCrop(input_size),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["# Load Data\n","\n","# configure paths to the description txt file and the images folder\n","#train_txt_file = '/kaggle/input/ismdatasetforclassificationofdieases/ism_dataset/raw_data/train_multi.txt'\n","#train_images_path = '/kaggle/input/ismdatasetforclassificationofdieases/ism_dataset/raw_data/train'\n","\n","#test_txt_file = '../COVID-Net/test_split_v2.txt'\n","#test_images_path = '../covid-chestxray-dataset/data/test'\n","\n","#train_dataset = covidx.COVIDxDataset(train_txt_file, train_images_path)\n","#test_dataset = covidx.COVIDxDataset(val_txt_file, val_images_path)\n","\n","print(\"Initializing Datasets and Dataloaders...\")\n","\n","# Create training and validation datasets\n","batch_size = 128\n","val_size = 150\n","\n","#training_dataset = COVIDxDataset(train_txt_file, train_images_path)\n","#val_size =  math.floor(len(training_dataset)*0.3)\n","#train_size = len(training_dataset) - val_size\n","#train_data,val_data = random_split(training_dataset,[train_size,val_size])\n","\n","#image_datasets = {'train': train_data, 'val': val_data}\n","#image_datasets = {'train': covidx.COVIDxDataset(train_txt_file, train_images_path, data_transforms['train']), 'val': covidx.COVIDxDataset(test_txt_file, test_images_path, data_transforms['val'])}\n","#image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n","# Create training and validation dataloaders\n","#dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=0) for x in ['train', 'val']}\n","\n","#print(dataloaders_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["\n","# Detect if we have a GPU available\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["# create the optimizer\n","\n","#model_ft = model_ft.to(device)\n","\n","#params_to_update = model_ft.parameters()\n","#print(\"Params to learn\")\n","\n","#if feature_extract:\n","#    params_to_update = []\n","#    for name, param in model_ft.named_parameters():\n","#        if param.requires_grad == True:\n","#            params_to_update.append(param)\n","#            print(\"\\t\", name)\n","#else:\n","#    for name, param in model_ft.named_parameters():\n"," #       if param.requires_grad == True:\n","  #          print(\"\\t\", name)\n"," #           \n","#optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum = 0.9)\n","#criterion = nn.CrossEntropyLoss()\n","\n","#model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs = num_epochs, is_inception=(model_name==\"inception\"))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"1245e537f49f36515f2fe2ada6417b5ef8f3abea8876a4d93eabac15ef0e31b7"}}},"nbformat":4,"nbformat_minor":4}
